{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T21:34:59.977454Z",
     "start_time": "2025-11-21T21:34:59.000202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import requests\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ],
   "id": "36b6abb20be05819",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T21:38:16.826052Z",
     "start_time": "2025-11-21T21:38:16.820025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Root directory where all data will be stored.\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "DATA_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Subdirectories:\n",
    "# - downloads: raw .zip.001, .zip.002, ... files\n",
    "# - raw: extracted content from SD-Tooth.zip\n",
    "# - processed_2d: cleaned & reorganized 2D images\n",
    "DOWNLOAD_DIR = DATA_ROOT / \"downloads\"\n",
    "RAW_DIR = DATA_ROOT / \"raw\"\n",
    "PROCESSED_2D_DIR = DATA_ROOT / \"processed_2d\"\n",
    "\n",
    "DOWNLOAD_DIR.mkdir(exist_ok=True)\n",
    "RAW_DIR.mkdir(exist_ok=True)\n",
    "PROCESSED_2D_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)"
   ],
   "id": "457bc07bae6fa9ef",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T22:49:51.622687Z",
     "start_time": "2025-11-21T21:51:09.187742Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download all 15 split zip parts\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1 (faster): Download all 15 split zip parts in parallel\n",
    "# \n",
    "# This is an accelerated version of the download step.\n",
    "# \n",
    "# Main changes compared to the original version:\n",
    "# - We use a ThreadPoolExecutor to download several parts in parallel.\n",
    "# - We use a single `requests.Session()` object (connection reuse).\n",
    "# - Already existing files are still skipped.\n",
    "# \n",
    "# Note:\n",
    "# - If your network is unstable, you can reduce `MAX_WORKERS` (e.g. 2).\n",
    "# - If your bandwidth is good, you can try increasing it to 6â€“8.\n",
    "\n",
    "# %%\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import itertools\n",
    "\n",
    "# Zenodo record ID for STS-Tooth\n",
    "ZENODO_RECORD_ID = \"10597292\"\n",
    "\n",
    "# Expected MD5 checksums from the Zenodo page.\n",
    "EXPECTED_MD5: Dict[str, str] = {\n",
    "    \"SD-Tooth.zip.001\": \"9fff54c469d2f5706332d01fd362178e\",\n",
    "    \"SD-Tooth.zip.002\": \"eb5be6735f8a20b469bef46a26784f3c\",\n",
    "    \"SD-Tooth.zip.003\": \"1ee5ce98fbfadeb48b2c264ac425ff20\",\n",
    "    \"SD-Tooth.zip.004\": \"12b7b965b1d7330c10850a40324368fd\",\n",
    "    \"SD-Tooth.zip.005\": \"1348eaf36bed8da9aabd5748a70a3d51\",\n",
    "    \"SD-Tooth.zip.006\": \"f0a2158d0cc915507ccefc53067b6a74\",\n",
    "    \"SD-Tooth.zip.007\": \"d728cec1d10e05845d33d2a3480c1cf9\",\n",
    "    \"SD-Tooth.zip.008\": \"af917049b72b512c4eae0d2801811b16\",\n",
    "    \"SD-Tooth.zip.009\": \"699713928665b2c15537d665689c7bdd\",\n",
    "    \"SD-Tooth.zip.010\": \"6ba5b3100fd516f031b68bc13868e0bb\",\n",
    "    \"SD-Tooth.zip.011\": \"82e5df4d8dda37a45ea1725b6b21942b\",\n",
    "    \"SD-Tooth.zip.012\": \"d7896f01462a4a1589f9f006e3c97190\",\n",
    "    \"SD-Tooth.zip.013\": \"763728b43b93cf081cc28ca15d4490a1\",\n",
    "    \"SD-Tooth.zip.014\": \"9cba0607e1d8066cd5aaac5c8091ee6a\",\n",
    "    \"SD-Tooth.zip.015\": \"0be8577f58e8bc1937f8e162887c4654\",\n",
    "}\n",
    "\n",
    "\n",
    "def build_part_filename(idx: int) -> str:\n",
    "    \"\"\"\n",
    "    Build a local filename for the given index.\n",
    "\n",
    "    Example:\n",
    "        idx = 1 -> \"SD-Tooth.zip.001\"\n",
    "        idx = 2 -> \"SD-Tooth.zip.002\"\n",
    "    \"\"\"\n",
    "    return f\"SD-Tooth.zip.{idx:03d}\"\n",
    "\n",
    "\n",
    "def build_part_url(idx: int) -> str:\n",
    "    \"\"\"\n",
    "    Build the remote download URL for a given part.\n",
    "\n",
    "    This follows the Zenodo file download pattern:\n",
    "        https://zenodo.org/records/<record_id>/files/<filename>?download=1\n",
    "    \"\"\"\n",
    "    part_name = build_part_filename(idx)\n",
    "    return f\"https://zenodo.org/records/{ZENODO_RECORD_ID}/files/{part_name}?download=1\"\n",
    "\n",
    "\n",
    "def md5sum(path: Path, chunk_size: int = 1024 * 1024) -> str:\n",
    "    \"\"\"\n",
    "    Compute the MD5 checksum of a file in a memory-efficient way.\n",
    "    \"\"\"\n",
    "    h = hashlib.md5()\n",
    "    with path.open(\"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(chunk_size), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "\n",
    "# Create a session to reuse HTTP connections across downloads\n",
    "_session = requests.Session()\n",
    "\n",
    "\n",
    "def download_file(url: str, dst: Path, chunk_size: int = 4 * 1024 * 1024) -> None:\n",
    "    \"\"\"\n",
    "    Download a file from `url` to `dst`, showing a progress bar.\n",
    "\n",
    "    If the destination file already exists, the download is skipped.\n",
    "\n",
    "    Args:\n",
    "        url: Remote URL.\n",
    "        dst: Local file path.\n",
    "        chunk_size: Number of bytes read per iteration (4 MB by default).\n",
    "    \"\"\"\n",
    "    if dst.exists():\n",
    "        print(f\"[skip] file already exists: {dst.name}\")\n",
    "        return\n",
    "\n",
    "    print(f\"[download] {dst.name} <- {url}\")\n",
    "    with _session.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        total = int(r.headers.get(\"Content-Length\", 0)) or None\n",
    "        with dst.open(\"wb\") as f, tqdm(\n",
    "            total=total, unit=\"B\", unit_scale=True, desc=dst.name\n",
    "        ) as pbar:\n",
    "            for chunk in r.iter_content(chunk_size=chunk_size):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "\n",
    "def download_one_part(idx: int):\n",
    "    \"\"\"\n",
    "    Helper wrapper for ThreadPoolExecutor.\n",
    "\n",
    "    It builds the filename and URL for a given part index and then\n",
    "    calls `download_file`.\n",
    "    \"\"\"\n",
    "    filename = build_part_filename(idx)\n",
    "    url = build_part_url(idx)\n",
    "    dst = DOWNLOAD_DIR / filename\n",
    "    download_file(url, dst)\n",
    "    return filename\n",
    "\n",
    "\n",
    "# Number of parallel workers (you can tune this)\n",
    "MAX_WORKERS = 6\n",
    "\n",
    "print(f\"Starting parallel download with {MAX_WORKERS} workers...\")\n",
    "\n",
    "# We wrap the executor.map call with tqdm so we see progress over the 15 parts.\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    list(\n",
    "        tqdm(\n",
    "            executor.map(download_one_part, range(1, 16)),\n",
    "            total=15,\n",
    "            desc=\"Downloading parts\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"All parts have finished the download step (existing files were skipped).\")\n"
   ],
   "id": "acea8e5400b1c43",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T23:39:49.997786Z",
     "start_time": "2025-11-21T23:39:06.881551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optional MD5 verification\n",
    "def verify_md5_all() -> bool:\n",
    "    \"\"\"\n",
    "    Verify the MD5 checksum of all downloaded parts.\n",
    "\n",
    "    Returns:\n",
    "        True if all files are present and all MD5 checksums match,\n",
    "        False otherwise.\n",
    "    \"\"\"\n",
    "    ok = True\n",
    "    for name, expected in EXPECTED_MD5.items():\n",
    "        path = DOWNLOAD_DIR / name\n",
    "        if not path.exists():\n",
    "            print(f\"[missing] {name}\")\n",
    "            ok = False\n",
    "            continue\n",
    "\n",
    "        real = md5sum(path)\n",
    "        if real.lower() == expected.lower():\n",
    "            print(f\"[OK] {name} md5 matches\")\n",
    "        else:\n",
    "            print(f\"[ERR] {name} md5 mismatch: got={real}, expected={expected}\")\n",
    "            ok = False\n",
    "    return ok\n",
    "\n",
    "\n",
    "# You can comment out the next two lines if you don't need verification.\n",
    "all_good = verify_md5_all()\n",
    "print(\"MD5 verification result:\", all_good)"
   ],
   "id": "9dd91cc9304f4bb2",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-21T23:41:51.229993Z",
     "start_time": "2025-11-21T23:40:31.398463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Merge parts and unzip\n",
    "\n",
    "# Path to the merged zip file\n",
    "MERGED_ZIP = DOWNLOAD_DIR / \"SD-Tooth.zip\"\n",
    "\n",
    "\n",
    "def merge_parts_to_zip():\n",
    "    \"\"\"\n",
    "    Merge all split zip parts into a single `SD-Tooth.zip` file.\n",
    "\n",
    "    Each split part is appended in order:\n",
    "        SD-Tooth.zip.001, SD-Tooth.zip.002, ..., SD-Tooth.zip.015\n",
    "\n",
    "    If the merged zip already exists, this step is skipped.\n",
    "    \"\"\"\n",
    "    if MERGED_ZIP.exists():\n",
    "        print(f\"[skip] merged zip already exists: {MERGED_ZIP}\")\n",
    "        return\n",
    "\n",
    "    print(\"[merge] merging parts into SD-Tooth.zip ...\")\n",
    "    with MERGED_ZIP.open(\"wb\") as out_f:\n",
    "        for i in range(1, 16):\n",
    "            part_path = DOWNLOAD_DIR / build_part_filename(i)\n",
    "            if not part_path.exists():\n",
    "                raise FileNotFoundError(f\"Missing split part: {part_path}\")\n",
    "            print(f\"  -> merging {part_path.name}\")\n",
    "            with part_path.open(\"rb\") as in_f:\n",
    "                shutil.copyfileobj(in_f, out_f)\n",
    "    print(\"[done] merge finished.\")\n",
    "\n",
    "\n",
    "merge_parts_to_zip()\n",
    "\n",
    "\n",
    "def unzip_merged_zip():\n",
    "    \"\"\"\n",
    "    Extract the merged `SD-Tooth.zip` into RAW_DIR.\n",
    "\n",
    "    If RAW_DIR already contains some files or directories,\n",
    "    we assume extraction has been done before and skip it.\n",
    "    \"\"\"\n",
    "    if any(RAW_DIR.iterdir()):\n",
    "        print(\"[skip] RAW_DIR is not empty, assuming data already extracted:\", RAW_DIR)\n",
    "        return\n",
    "\n",
    "    print(f\"[unzip] {MERGED_ZIP} -> {RAW_DIR}\")\n",
    "    with zipfile.ZipFile(MERGED_ZIP, \"r\") as zf:\n",
    "        zf.extractall(RAW_DIR)\n",
    "    print(\"[done] unzip finished.\")\n",
    "\n",
    "\n",
    "unzip_merged_zip()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3: Locate the `STS-2D-Tooth` directory and scan all PNGs\n",
    "# \n",
    "# The extracted folder may contain multiple top-level directories.  \n",
    "# We need to:\n",
    "# \n",
    "# 1. Search for the directory whose name contains `\"STS-2D-Tooth\"`.\n",
    "# 2. Recursively scan all `.png` files under this directory.\n",
    "# 3. For each PNG file, infer:\n",
    "#    - `age_group`: `\"adult\"`, `\"children\"`, or `\"unknown\"` based on the path (`A-PXI` / `C-PXI`).\n",
    "#    - `label_status`: `\"labeled\"`, `\"unlabeled\"`, or `\"unknown\"` based on path (`Labeled` / `Unlabeled`).\n",
    "#    - `is_mask`: Boolean, `True` if the filename suggests a mask (`mask`/`label`).\n",
    "#    - `pair_id`: A normalized ID used to match an image and its mask (e.g., `A001` vs `A001_mask`).\n",
    "# 4. Save all this information into a `pandas.DataFrame` and write it to `sts2d_index.csv`."
   ],
   "id": "79058cd703e68b5d",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:00:24.236626Z",
     "start_time": "2025-11-22T00:00:23.894381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#  Find STS-2D-Tooth root & scan PNGs\n",
    "# %%  (fixed Cell 5) Find STS-2D-Tooth root & scan PNGs\n",
    "\n",
    "def find_sts2d_root(raw_root: Path) -> Path:\n",
    "    candidates: List[Path] = []\n",
    "    for p in raw_root.rglob(\"*\"):\n",
    "        if p.is_dir() and \"STS-2D-Tooth\" in p.name:\n",
    "            candidates.append(p)\n",
    "\n",
    "    if not candidates:\n",
    "        raise RuntimeError(\n",
    "            \"Could not find a directory with name containing 'STS-2D-Tooth' under RAW_DIR.\"\n",
    "        )\n",
    "\n",
    "    candidates.sort(key=lambda x: len(str(x)))\n",
    "    sts2d_root = candidates[0]\n",
    "    print(\"[found STS-2D-Tooth directory]:\", sts2d_root)\n",
    "    return sts2d_root\n",
    "\n",
    "\n",
    "STS2D_ROOT = find_sts2d_root(RAW_DIR)\n",
    "\n",
    "\n",
    "def infer_age_group(path: Path) -> str:\n",
    "    s = str(path)\n",
    "    if \"A-PXI\" in s:\n",
    "        return \"adult\"\n",
    "    if \"C-PXI\" in s:\n",
    "        return \"children\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def infer_label_status(path: Path) -> str:\n",
    "    s = str(path)\n",
    "    if \"Labeled\" in s:\n",
    "        return \"labeled\"\n",
    "    if \"Unlabeled\" in s:\n",
    "        return \"unlabeled\"\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def infer_is_mask(path: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Heuristically decide whether this PNG is a mask file.\n",
    "\n",
    "    We now check BOTH:\n",
    "    - file name patterns (e.g. *_mask.png, *-label.png)\n",
    "    - directory names like 'Mask', 'Masks', 'Label', 'Labels'\n",
    "\n",
    "    This is to correctly catch masks that are placed under a 'Mask' folder\n",
    "    but whose file names do not contain 'mask' or 'label'.\n",
    "    \"\"\"\n",
    "    stem = path.stem.lower()\n",
    "    # 1) file name patterns\n",
    "    for suf in [\"_mask\", \"-mask\", \"_label\", \"-label\"]:\n",
    "        if stem.endswith(suf):\n",
    "            return True\n",
    "\n",
    "    # 2) directory names\n",
    "    parts = [p.lower() for p in path.parts]\n",
    "    if any(p in {\"mask\", \"masks\", \"label\", \"labels\"} for p in parts):\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def make_pair_id(path: Path) -> str:\n",
    "    stem = path.stem.lower()\n",
    "    for suf in [\"_mask\", \"-mask\", \"_label\", \"-label\"]:\n",
    "        if stem.endswith(suf):\n",
    "            stem = stem[: -len(suf)]\n",
    "            break\n",
    "    return stem\n",
    "\n",
    "\n",
    "records = []\n",
    "\n",
    "print(\"[scan] scanning all PNG files under STS-2D-Tooth ...\")\n",
    "for png_path in tqdm(list(STS2D_ROOT.rglob(\"*.png\"))):\n",
    "    rel_path = png_path.relative_to(STS2D_ROOT)\n",
    "    age_group = infer_age_group(png_path)\n",
    "    label_status = infer_label_status(png_path)\n",
    "    is_mask = infer_is_mask(png_path)\n",
    "    pair_id = make_pair_id(png_path)\n",
    "\n",
    "    records.append(\n",
    "        {\n",
    "            \"rel_path\": str(rel_path).replace(\"\\\\\", \"/\"),\n",
    "            \"age_group\": age_group,\n",
    "            \"label_status\": label_status,\n",
    "            \"is_mask\": is_mask,\n",
    "            \"pair_id\": pair_id,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "print(\"Total number of PNG files found:\", len(df))\n",
    "\n",
    "INDEX_CSV = DATA_ROOT / \"sts2d_index.csv\"\n",
    "df.to_csv(INDEX_CSV, index=False)\n",
    "print(\"Index file saved to:\", INDEX_CSV)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 3.1: Quick sanity checks and statistics\n",
    "# \n",
    "# - Check how many files belong to each `age_group`.\n",
    "# - Check how many files are labeled vs unlabeled.\n",
    "# - Check how many files are masks vs normal images.\n",
    "# - Show the first few rows of `pair_id` statistics to see if images and masks are paired correctly."
   ],
   "id": "c41a6b3086e0fb50",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:00:28.218294Z",
     "start_time": "2025-11-22T00:00:28.207116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Basic statistics for sanity check\n",
    "print(\"\\n=== age_group distribution ===\")\n",
    "print(df[\"age_group\"].value_counts())\n",
    "\n",
    "print(\"\\n=== label_status distribution ===\")\n",
    "print(df[\"label_status\"].value_counts())\n",
    "\n",
    "print(\"\\n=== is_mask distribution ===\")\n",
    "print(df[\"is_mask\"].value_counts())\n",
    "\n",
    "# Group by pair_id and is_mask to see how many images/masks each pair_id has\n",
    "pair_stats = (\n",
    "    df.groupby([\"pair_id\", \"is_mask\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename(columns={False: \"num_image_like\", True: \"num_mask_like\"})\n",
    ")\n",
    "\n",
    "print(\"\\nExample of pairing statistics (first 10 rows):\")\n",
    "print(pair_stats.head(10))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 4: Organize 2D images into a clean directory structure\n",
    "# \n",
    "# For easier training and data loading, we copy (or link) the images into:\n",
    "# \n",
    "# ```text\n",
    "# processed_2d/\n",
    "#   adult/\n",
    "#     labeled/\n",
    "#       images/\n",
    "#       masks/\n",
    "#     unlabeled/\n",
    "#       images/\n",
    "#   children/\n",
    "#     labeled/\n",
    "#       images/\n",
    "#       masks/\n",
    "#     unlabeled/\n",
    "#       images/\n",
    "#   unknown/\n",
    "#     ...\n",
    "# ```\n",
    "# \n",
    "# Notes:\n",
    "# - `age_group` values other than `\"adult\"`/`\"children\"` are mapped to `\"unknown\"`.\n",
    "# - `label_status` values other than `\"labeled\"`/`\"unlabeled\"` are mapped to `\"unknown\"`.\n",
    "# - You can disable the copying by setting `ENABLE_COPY = False`."
   ],
   "id": "bca40a1d6632c62",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:00:37.698239Z",
     "start_time": "2025-11-22T00:00:31.141805Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Copy files into processed_2d directory\n",
    "\n",
    "# Whether to actually copy files.\n",
    "# If the dataset is too large and you don't want duplication,\n",
    "# you can set this to False and only use the CSV index.\n",
    "ENABLE_COPY = True\n",
    "\n",
    "\n",
    "def safe_copy(src: Path, dst: Path):\n",
    "    \"\"\"\n",
    "    Copy file from src to dst, creating parent directories if needed.\n",
    "\n",
    "    If dst already exists, the copy is skipped.\n",
    "    \"\"\"\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists():\n",
    "        return\n",
    "    shutil.copy2(src, dst)\n",
    "\n",
    "\n",
    "if ENABLE_COPY:\n",
    "    print(\"\\n[organize] copying 2D images into processed_2d by age_group / label_status / is_mask ...\")\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        rel_path = Path(row[\"rel_path\"])\n",
    "        age_group = row[\"age_group\"]\n",
    "        label_status = row[\"label_status\"]\n",
    "        is_mask = bool(row[\"is_mask\"])\n",
    "\n",
    "        src = STS2D_ROOT / rel_path\n",
    "\n",
    "        # Handle unknown values: group them into 'unknown'\n",
    "        if age_group not in (\"adult\", \"children\"):\n",
    "            age_group = \"unknown\"\n",
    "        if label_status not in (\"labeled\", \"unlabeled\"):\n",
    "            label_status = \"unknown\"\n",
    "\n",
    "        # Decide which subfolder to use: 'images' or 'masks'\n",
    "        sub = \"masks\" if is_mask else \"images\"\n",
    "\n",
    "        # Target path:\n",
    "        # processed_2d / {age_group} / {label_status} / images|masks / <filename>\n",
    "        dst = PROCESSED_2D_DIR / age_group / label_status / sub / rel_path.name\n",
    "        safe_copy(src, dst)\n",
    "\n",
    "    print(\"[done] 2D images have been organized under:\", PROCESSED_2D_DIR)\n",
    "else:\n",
    "    print(\"\\n[skip] ENABLE_COPY is False; no files were copied.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Summary\n",
    "# \n",
    "# - `INDEX_CSV` contains a full index of all 2D PNG files, with:\n",
    "#   * `rel_path` (relative to `STS2D_ROOT`)\n",
    "#   * `age_group` (`adult` / `children` / `unknown`)\n",
    "#   * `label_status` (`labeled` / `unlabeled` / `unknown`)\n",
    "#   * `is_mask` (True / False)\n",
    "#   * `pair_id` (used to link images with masks)\n",
    "# - `STS2D_ROOT` is the root of the original `STS-2D-Tooth` folder.\n",
    "# - `PROCESSED_2D_DIR` contains the reorganized dataset, ready for training.\n"
   ],
   "id": "593d6083c9af28b5",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:00:43.666186Z",
     "start_time": "2025-11-22T00:00:43.663310Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Final summary printout\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(\"Index CSV file:\", INDEX_CSV)\n",
    "print(\"Original 2D root directory:\", STS2D_ROOT)\n",
    "print(\"Processed 2D directory:\", PROCESSED_2D_DIR)\n",
    "print(\"You can now load the CSV with pandas and join `STS2D_ROOT` + `rel_path` to get absolute paths for training.\")"
   ],
   "id": "9b73d23637e8ce57",
   "execution_count": 15,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
