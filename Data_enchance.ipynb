{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-22T00:01:01.408998Z",
     "start_time": "2025-11-22T00:01:01.391999Z"
    }
   },
   "source": [
    "# %%  Cell 1 — pick a pair (image + mask) that really has a mask\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "INDEX_CSV = DATA_ROOT / \"sts2d_index.csv\"\n",
    "\n",
    "# 1) load index\n",
    "df = pd.read_csv(INDEX_CSV)\n",
    "\n",
    "# 2) compute how many image-like and mask-like files each pair_id has\n",
    "pair_stats = (\n",
    "    df.groupby([\"pair_id\", \"is_mask\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename(columns={False: \"num_image_like\", True: \"num_mask_like\"})\n",
    ")\n",
    "\n",
    "# 3) keep only those pair_ids that have at least 1 image and 1 mask\n",
    "valid_pair_ids = pair_stats[\n",
    "    (pair_stats.get(\"num_image_like\", 0) > 0)\n",
    "    & (pair_stats.get(\"num_mask_like\", 0) > 0)\n",
    "].index\n",
    "\n",
    "print(\"Number of pairs with BOTH image and mask:\", len(valid_pair_ids))\n",
    "\n",
    "# 4) randomly choose one valid pair_id\n",
    "sample_pair_id = pd.Series(list(valid_pair_ids)).sample(1, random_state=0).iloc[0]\n",
    "print(\"Chosen pair_id:\", sample_pair_id)\n",
    "\n",
    "# 5) get rows for this pair_id\n",
    "sample_rows = df[df[\"pair_id\"] == sample_pair_id]\n",
    "\n",
    "img_row = sample_rows[sample_rows[\"is_mask\"] == False].iloc[0]\n",
    "mask_row = sample_rows[sample_rows[\"is_mask\"] == True].iloc[0]\n",
    "\n",
    "img_rel      = img_row[\"rel_path\"]      # relative path of image (inside STS-2D-Tooth)\n",
    "mask_rel     = mask_row[\"rel_path\"]     # relative path of mask  (inside STS-2D-Tooth)\n",
    "age_group    = img_row[\"age_group\"]     # 'adult' or 'children'\n",
    "label_status = img_row[\"label_status\"]  # should be 'labeled'\n",
    "\n",
    "print(\"img_rel     :\", img_rel)\n",
    "print(\"mask_rel    :\", mask_rel)\n",
    "print(\"age_group   :\", age_group)\n",
    "print(\"label_status:\", label_status)\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:01:06.145133Z",
     "start_time": "2025-11-22T00:01:05.628460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%  Cell 2 — visualize image + mask + overlay\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "PROCESSED_2D_DIR = DATA_ROOT / \"processed_2d\"\n",
    "\n",
    "# build absolute paths inside processed_2d\n",
    "img_path  = PROCESSED_2D_DIR / age_group / label_status / \"images\" / Path(img_rel).name\n",
    "mask_path = PROCESSED_2D_DIR / age_group / label_status / \"masks\"  / Path(mask_rel).name\n",
    "\n",
    "print(\"Image path:\", img_path)\n",
    "print(\"Mask path :\", mask_path)\n",
    "\n",
    "if not img_path.exists():\n",
    "    raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "if not mask_path.exists():\n",
    "    raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
    "\n",
    "img  = Image.open(img_path).convert(\"L\")\n",
    "mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(img, cmap=\"gray\")\n",
    "axes[0].set_title(\"Original image\")\n",
    "axes[0].axis(\"off\")\n",
    "\n",
    "axes[1].imshow(mask, cmap=\"gray\")\n",
    "axes[1].set_title(\"Mask\")\n",
    "axes[1].axis(\"off\")\n",
    "\n",
    "axes[2].imshow(img, cmap=\"gray\")\n",
    "axes[2].imshow(mask, alpha=0.4)\n",
    "axes[2].set_title(\"Overlay\")\n",
    "axes[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "7febe3bc921b1233",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:31:44.924891Z",
     "start_time": "2025-11-22T00:31:44.917614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports and global configuration\n",
    "from pathlib import Path\n",
    "import random\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Make plots appear inline (Jupyter)\n",
    "# (PyCharm / VSCode may ignore this, which is fine)\n",
    "%matplotlib inline  \n",
    "\n",
    "# Set some global seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Root directory used in preprocessing.ipynb\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "PROCESSED_2D_DIR = DATA_ROOT / \"processed_2d\"\n",
    "INDEX_CSV = DATA_ROOT / \"sts2d_index.csv\"\n",
    "\n",
    "print(\"DATA_ROOT       :\", DATA_ROOT)\n",
    "print(\"PROCESSED_2D_DIR:\", PROCESSED_2D_DIR)\n",
    "print(\"INDEX_CSV       :\", INDEX_CSV)\n",
    "\n",
    "assert INDEX_CSV.exists(), \"Index CSV not found. Please run preprocessing.ipynb first.\"\n",
    "assert PROCESSED_2D_DIR.exists(), \"processed_2d directory not found. Please run preprocessing.ipynb first.\"\n"
   ],
   "id": "fa4251cd0fc949a",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:33:59.366711Z",
     "start_time": "2025-11-22T00:33:59.312675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load index and build image / mask / pair tables\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV)\n",
    "\n",
    "print(\"Full index shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nValue counts — age_group:\")\n",
    "print(df[\"age_group\"].value_counts())\n",
    "\n",
    "print(\"\\nValue counts — label_status:\")\n",
    "print(df[\"label_status\"].value_counts())\n",
    "\n",
    "print(\"\\nValue counts — is_mask:\")\n",
    "print(df[\"is_mask\"].value_counts())\n",
    "\n",
    "# Split into images and masks\n",
    "df_img = df[df[\"is_mask\"] == False].copy()\n",
    "df_mask = df[df[\"is_mask\"] == True].copy()\n",
    "\n",
    "print(\"\\nNumber of image rows:\", len(df_img))\n",
    "print(\"Number of mask rows :\", len(df_mask))\n",
    "\n",
    "# Build a table of pair_ids that have at least one image and one mask\n",
    "pair_stats = (\n",
    "    df.groupby([\"pair_id\", \"is_mask\"])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .rename(columns={False: \"num_image_like\", True: \"num_mask_like\"})\n",
    ")\n",
    "\n",
    "num_pairs_with_both = (pair_stats[\"num_image_like\"] > 0) & (pair_stats[\"num_mask_like\"] > 0)\n",
    "print(\"\\nNumber of pair_ids with BOTH image and mask:\", num_pairs_with_both.sum())\n",
    "\n",
    "# Build a segmentation dataframe: one row per image that has a mask\n",
    "df_seg = pd.merge(\n",
    "    df_img,\n",
    "    df_mask[[\"pair_id\", \"rel_path\"]].rename(columns={\"rel_path\": \"mask_rel\"}),\n",
    "    on=\"pair_id\",\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "print(\"\\nSegmentation dataframe shape:\", df_seg.shape)\n",
    "print(df_seg.head(3))\n",
    "\n",
    "# For pretraining (Stage 1), we use ALL non-mask images (df_img)\n",
    "df_pretrain = df_img.copy()\n",
    "print(\"\\nPretraining dataframe shape (all images, mask images excluded):\", df_pretrain.shape)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Helper functions for reading images and masks\n",
    "# \n",
    "# Now we define small utility functions to:\n",
    "# \n",
    "# - Map a row from `df_pretrain` / `df_seg` to an actual image file under `processed_2d`.\n",
    "# - Load images and masks as NumPy arrays suitable for Albumentations.\n",
    "# \n",
    "# Directory layout under `processed_2d` (from preprocessing):\n",
    "# \n",
    "# ```text\n",
    "# processed_2d/\n",
    "#   adult/\n",
    "#     labeled/\n",
    "#       images/\n",
    "#       masks/\n",
    "#     unlabeled/\n",
    "#       images/\n",
    "#   children/\n",
    "#     labeled/\n",
    "#       images/\n",
    "#       masks/\n",
    "#     unlabeled/\n",
    "#       images/\n",
    "#   unknown/ ...\n",
    "# ```"
   ],
   "id": "c83ad75c4fd98ea4",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T00:38:58.370689Z",
     "start_time": "2025-11-22T00:38:58.048816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path mapping and image loading helpers\n",
    "\n",
    "def get_image_path_from_row(row: pd.Series) -> Path:\n",
    "    \"\"\"\n",
    "    Build the path to an image file under processed_2d from a DataFrame row.\n",
    "\n",
    "    For images, we use:\n",
    "        processed_2d / age_group / label_status / \"images\" / <filename>\n",
    "\n",
    "    where <filename> is taken from the original rel_path.\n",
    "    \"\"\"\n",
    "    age_group = row[\"age_group\"] if isinstance(row[\"age_group\"], str) else \"unknown\"\n",
    "    label_status = row[\"label_status\"] if isinstance(row[\"label_status\"], str) else \"unknown\"\n",
    "\n",
    "    filename = Path(row[\"rel_path\"]).name\n",
    "    img_path = PROCESSED_2D_DIR / age_group / label_status / \"images\" / filename\n",
    "    return img_path\n",
    "\n",
    "\n",
    "def get_mask_path_from_row(row: pd.Series) -> Path:\n",
    "    \"\"\"\n",
    "    Build the path to a mask file under processed_2d from a segmentation row.\n",
    "\n",
    "    df_seg has a column 'mask_rel' which stores the original relative path of the mask.\n",
    "    We again map it to:\n",
    "        processed_2d / age_group / label_status / \"masks\" / <mask_filename>\n",
    "    \"\"\"\n",
    "    age_group = row[\"age_group\"] if isinstance(row[\"age_group\"], str) else \"unknown\"\n",
    "    label_status = row[\"label_status\"] if isinstance(row[\"label_status\"], str) else \"unknown\"\n",
    "\n",
    "    filename = Path(row[\"mask_rel\"]).name\n",
    "    mask_path = PROCESSED_2D_DIR / age_group / label_status / \"masks\" / filename\n",
    "    return mask_path\n",
    "\n",
    "\n",
    "def load_image_as_array(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load a grayscale PNG as a NumPy array with shape (H, W).\n",
    "\n",
    "    We use OpenCV in grayscale mode (0), which returns a 2D array of type uint8.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    img = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise RuntimeError(f\"Failed to read image: {path}\")\n",
    "    return img\n",
    "\n",
    "\n",
    "def load_mask_as_array(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load a segmentation mask as a NumPy array with shape (H, W).\n",
    "\n",
    "    Masks are usually stored as uint8 (0, 1, 2, ...). We keep them as integers\n",
    "    because they represent class indices, not intensities.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Mask not found: {path}\")\n",
    "    mask = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    if mask is None:\n",
    "        raise RuntimeError(f\"Failed to read mask: {path}\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Quick sanity check: load and visualize one segmentation pair\n",
    "example_row = df_seg.sample(1, random_state=SEED).iloc[0]\n",
    "\n",
    "example_img_path = get_image_path_from_row(example_row)\n",
    "example_mask_path = get_mask_path_from_row(example_row)\n",
    "\n",
    "print(\"Example image path:\", example_img_path)\n",
    "print(\"Example mask path :\", example_mask_path)\n",
    "\n",
    "example_img = load_image_as_array(example_img_path)\n",
    "example_mask = load_mask_as_array(example_mask_path)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(example_img, cmap=\"gray\")\n",
    "plt.title(\"Original image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(example_mask, cmap=\"gray\")\n",
    "plt.title(\"Original mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(example_img, cmap=\"gray\")\n",
    "plt.imshow(example_mask, alpha=0.4)\n",
    "plt.title(\"Overlay\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Define augmentation configurations\n",
    "# \n",
    "# We will use **Albumentations** for flexible, image-friendly augmentation.\n",
    "# \n",
    "# We define three presets:\n",
    "# \n",
    "# 1. **Segmentation training (`seg_train_transform`)**\n",
    "#    - Mild to moderate geometric transforms (shift/scale/rotate, crop, flip).\n",
    "#    - Mild brightness/contrast and noise.\n",
    "#    - Applied to both image and mask (geometrically consistent).\n",
    "# \n",
    "# 2. **Weak view for pretraining (`ssl_weak_transform`)**\n",
    "#    - Similar to segmentation but slightly simpler.\n",
    "#    - Good for general supervised training as well.\n",
    "# \n",
    "# 3. **Strong view for pretraining (`ssl_strong_transform`)**\n",
    "#    - More aggressive cropping, color/intensity jitter, blur, and cutout.\n",
    "#    - Used only for Stage 1 self-supervised pretraining.\n",
    "# \n",
    "# **Important medical constraints:**\n",
    "# \n",
    "# - **No vertical flip** (we don't want to swap upper and lower jaws).\n",
    "# - Only **small rotations** (e.g. ±10–15 degrees).\n",
    "# - Cropping should not completely remove the dental arches.\n",
    "# - All geometric transforms must be applied consistently to image and mask."
   ],
   "id": "c14f5d51af342e22",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:07.583100Z",
     "start_time": "2025-11-22T01:21:07.574908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Albumentations transform builders\n",
    "\n",
    "TARGET_HEIGHT = 320\n",
    "TARGET_WIDTH = 640\n",
    "\n",
    "\n",
    "def get_segmentation_train_transform() -> A.Compose:\n",
    "    \"\"\"\n",
    "    Augmentation pipeline for supervised segmentation training.\n",
    "\n",
    "    This transform:\n",
    "    - Resizes the image to 320 x 640.\n",
    "    - Applies random horizontal flip.\n",
    "    - Applies mild Shift/Scale/Rotate.\n",
    "    - Applies RandomResizedCrop to introduce some cropping variability.\n",
    "    - Applies mild brightness/contrast changes and Gaussian noise.\n",
    "    - Finally converts to PyTorch tensors.\n",
    "\n",
    "    Notes:\n",
    "    - Geometric transforms are applied to both image and mask.\n",
    "    - Intensity transforms only affect images.\n",
    "    \"\"\"\n",
    "    return A.Compose(\n",
    "        [\n",
    "            # Ensure a consistent resolution\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "\n",
    "            # Geometric transforms (applied to both image & mask)\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.05,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=10,\n",
    "                border_mode=cv2.BORDER_REFLECT_101,\n",
    "                p=0.75,\n",
    "            ),\n",
    "            # New API: use size=(H, W)\n",
    "            A.RandomResizedCrop(\n",
    "                size=(TARGET_HEIGHT, TARGET_WIDTH),\n",
    "                scale=(0.8, 1.0),\n",
    "                ratio=(1.5, 2.5),\n",
    "                p=0.5,\n",
    "            ),\n",
    "\n",
    "            # Intensity transforms (image only)\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.15,\n",
    "                contrast_limit=0.15,\n",
    "                p=0.5,\n",
    "            ),\n",
    "            # Use default noise parameters (avoid var_limit warnings)\n",
    "            A.GaussNoise(p=0.3),\n",
    "\n",
    "            # Normalize to [0,1] then standardize.\n",
    "            A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "\n",
    "            # Convert to PyTorch tensors\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_segmentation_val_transform() -> A.Compose:\n",
    "    \"\"\"\n",
    "    Simple transform for validation / test.\n",
    "\n",
    "    Only resizes and normalizes, no random augmentation.\n",
    "    \"\"\"\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "            A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_ssl_weak_transform() -> A.Compose:\n",
    "    \"\"\"\n",
    "    Weak augmentation for self-supervised / pretraining.\n",
    "\n",
    "    Similar to segmentation training, but slightly simpler.\n",
    "    \"\"\"\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.05,\n",
    "                scale_limit=0.1,\n",
    "                rotate_limit=10,\n",
    "                border_mode=cv2.BORDER_REFLECT_101,\n",
    "                p=0.7,\n",
    "            ),\n",
    "\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.2,\n",
    "                contrast_limit=0.2,\n",
    "                p=0.5,\n",
    "            ),\n",
    "            A.GaussNoise(p=0.3),\n",
    "\n",
    "            A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_ssl_strong_transform() -> A.Compose:\n",
    "    \"\"\"\n",
    "    Strong augmentation for self-supervised / contrastive pretraining.\n",
    "\n",
    "    - Stronger cropping & color jitter.\n",
    "    - Additional blur and stronger noise.\n",
    "    - We no longer use CoarseDropout here.\n",
    "    \"\"\"\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "\n",
    "            A.RandomResizedCrop(\n",
    "                size=(TARGET_HEIGHT, TARGET_WIDTH),\n",
    "                scale=(0.5, 1.0),   # more aggressive cropping\n",
    "                ratio=(1.2, 3.0),\n",
    "                p=1.0,\n",
    "            ),\n",
    "\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.08,\n",
    "                scale_limit=0.15,\n",
    "                rotate_limit=15,\n",
    "                border_mode=cv2.BORDER_REFLECT_101,\n",
    "                p=0.8,\n",
    "            ),\n",
    "\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.3,\n",
    "                contrast_limit=0.3,\n",
    "                p=0.8,\n",
    "            ),\n",
    "\n",
    "            A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "            A.GaussNoise(p=0.5),\n",
    "\n",
    "            A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Albumentations transform builders\n",
    "# \n",
    "# We define 4 transforms:\n",
    "# \n",
    "# 1. `get_segmentation_train_transform()`\n",
    "# 2. `get_segmentation_val_transform()`\n",
    "# 3. `get_ssl_weak_transform()`  – weak augmentation for pretraining\n",
    "# 4. `get_ssl_strong_transform()` – stronger augmentation for pretraining\n",
    "# \n",
    "# Notes:\n",
    "# - We use the original image size 320 (H) x 640 (W).\n",
    "# - We do NOT use CoarseDropout here anymore.\n",
    "# - Masked Image Modeling (MIM) style masking will be handled separately later.\n"
   ],
   "id": "74d3dadee72c97a4",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T21:39:00.971536Z",
     "start_time": "2025-11-25T21:39:00.766814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preview segmentation augmentations\n",
    "seg_train_transform = get_segmentation_train_transform()\n",
    "\n",
    "NUM_SAMPLES = 2      # how many different pairs to visualize\n",
    "AUG_PER_SAMPLE = 3   # how many augmentations per pair\n",
    "\n",
    "seg_rows = df_seg.sample(NUM_SAMPLES, random_state=SEED)\n",
    "\n",
    "for idx, row in seg_rows.iterrows():\n",
    "    img_path = get_image_path_from_row(row)\n",
    "    mask_path = get_mask_path_from_row(row)\n",
    "\n",
    "    img = load_image_as_array(img_path)\n",
    "    mask = load_mask_as_array(mask_path)\n",
    "\n",
    "    print(\"\\n=== Example pair ===\")\n",
    "    print(\"Image path:\", img_path)\n",
    "    print(\"Mask path :\", mask_path)\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        AUG_PER_SAMPLE + 1, 3, figsize=(12, 4 * (AUG_PER_SAMPLE + 1))\n",
    "    )\n",
    "\n",
    "    # First row: original\n",
    "    axes[0, 0].imshow(img, cmap=\"gray\")\n",
    "    axes[0, 0].set_title(\"Original image\")\n",
    "    axes[0, 0].axis(\"off\")\n",
    "\n",
    "    axes[0, 1].imshow(mask, cmap=\"gray\")\n",
    "    axes[0, 1].set_title(\"Original mask\")\n",
    "    axes[0, 1].axis(\"off\")\n",
    "\n",
    "    axes[0, 2].imshow(img, cmap=\"gray\")\n",
    "    axes[0, 2].imshow(mask, alpha=0.4)\n",
    "    axes[0, 2].set_title(\"Original overlay\")\n",
    "    axes[0, 2].axis(\"off\")\n",
    "\n",
    "    # Augmented rows\n",
    "    for i in range(AUG_PER_SAMPLE):\n",
    "        transformed = seg_train_transform(image=img, mask=mask)\n",
    "        aug_img = transformed[\"image\"].squeeze(0).cpu().numpy()  # shape: (H, W)\n",
    "        aug_mask = transformed[\"mask\"].cpu().numpy()\n",
    "\n",
    "        r = i + 1\n",
    "        axes[r, 0].imshow(aug_img, cmap=\"gray\")\n",
    "        axes[r, 0].set_title(f\"Aug image #{i+1}\")\n",
    "        axes[r, 0].axis(\"off\")\n",
    "\n",
    "        axes[r, 1].imshow(aug_mask, cmap=\"gray\")\n",
    "        axes[r, 1].set_title(f\"Aug mask #{i+1}\")\n",
    "        axes[r, 1].axis(\"off\")\n",
    "\n",
    "        axes[r, 2].imshow(aug_img, cmap=\"gray\")\n",
    "        axes[r, 2].imshow(aug_mask, alpha=0.4)\n",
    "        axes[r, 2].set_title(f\"Aug overlay #{i+1}\")\n",
    "        axes[r, 2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Visualize pretraining (SSL) augmentations\n",
    "# \n",
    "# For Stage 1 pretraining, we:\n",
    "# \n",
    "# - Use **all** images in `df_pretrain` (both labeled and unlabeled, adult and children).\n",
    "# - Ignore masks completely.\n",
    "# - For each image, generate:\n",
    "#   * one **weak** view (`ssl_weak_transform`)\n",
    "#   * one **strong** view (`ssl_strong_transform`)\n",
    "# \n",
    "# In this cell we visualize a few examples to check:\n",
    "# \n",
    "# - Whether the two views clearly belong to the same underlying image.\n",
    "# - Whether the strong augmentations are sufficiently different but still realistic.\n",
    "\n"
   ],
   "id": "18fcd1ea257782e7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_segmentation_train_transform' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[1]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;66;03m# Preview segmentation augmentations\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m seg_train_transform = \u001B[43mget_segmentation_train_transform\u001B[49m()\n\u001B[32m      4\u001B[39m NUM_SAMPLES = \u001B[32m2\u001B[39m      \u001B[38;5;66;03m# how many different pairs to visualize\u001B[39;00m\n\u001B[32m      5\u001B[39m AUG_PER_SAMPLE = \u001B[32m3\u001B[39m   \u001B[38;5;66;03m# how many augmentations per pair\u001B[39;00m\n",
      "\u001B[31mNameError\u001B[39m: name 'get_segmentation_train_transform' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:21:25.239247Z",
     "start_time": "2025-11-22T01:21:24.432812Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Preview SSL weak + strong augmentations\n",
    "ssl_weak_transform = get_ssl_weak_transform()\n",
    "ssl_strong_transform = get_ssl_strong_transform()\n",
    "\n",
    "NUM_PRETRAIN_SAMPLES = 4\n",
    "\n",
    "pretrain_rows = df_pretrain.sample(NUM_PRETRAIN_SAMPLES, random_state=SEED)\n",
    "\n",
    "for idx, row in pretrain_rows.iterrows():\n",
    "    img_path = get_image_path_from_row(row)\n",
    "    img = load_image_as_array(img_path)\n",
    "\n",
    "    print(\"\\n=== Pretrain sample ===\")\n",
    "    print(\"Image path:\", img_path)\n",
    "    print(\"age_group   :\", row[\"age_group\"])\n",
    "    print(\"label_status:\", row[\"label_status\"])\n",
    "\n",
    "    # Apply weak & strong augmentations\n",
    "    out_weak = ssl_weak_transform(image=img)\n",
    "    weak_img = out_weak[\"image\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "    out_strong = ssl_strong_transform(image=img)\n",
    "    strong_img = out_strong[\"image\"].squeeze(0).cpu().numpy()\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(img, cmap=\"gray\")\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(weak_img, cmap=\"gray\")\n",
    "    plt.title(\"Weak view\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(strong_img, cmap=\"gray\")\n",
    "    plt.title(\"Strong view\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "d9e6f71cf5900c3c",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:28:32.280403Z",
     "start_time": "2025-11-22T01:28:32.234948Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Segmentation dataset\n",
    "\n",
    "class DentalSegmentationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for supervised segmentation.\n",
    "\n",
    "    Each item returns:\n",
    "        image: FloatTensor (1, H, W)\n",
    "        mask:  LongTensor (H, W)\n",
    "        meta:  dict with auxiliary information\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_seg: pd.DataFrame, transform: A.Compose):\n",
    "        self.df = df_seg.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = get_image_path_from_row(row)\n",
    "        mask_path = get_mask_path_from_row(row)\n",
    "\n",
    "        img = load_image_as_array(img_path)   # uint8, (H, W)\n",
    "        mask = load_mask_as_array(mask_path)  # uint8/int, (H, W)\n",
    "\n",
    "        # Albumentations expects dicts; we pass both image and mask.\n",
    "        transformed = self.transform(image=img, mask=mask)\n",
    "        img_t = transformed[\"image\"]          # FloatTensor (1, H, W)\n",
    "        mask_t = transformed[\"mask\"]          # tensor (H, W) or (1, H, W)\n",
    "\n",
    "        # Ensure mask is (H, W) LongTensor representing class indices\n",
    "        if mask_t.ndim == 3:\n",
    "            # shape (1, H, W) -> (H, W)\n",
    "            mask_t = mask_t.squeeze(0)\n",
    "        mask_t = mask_t.long()\n",
    "\n",
    "        meta = {\n",
    "            \"img_path\": str(img_path),\n",
    "            \"mask_path\": str(mask_path),\n",
    "            \"pair_id\": row[\"pair_id\"],\n",
    "            \"age_group\": row[\"age_group\"],\n",
    "            \"label_status\": row[\"label_status\"],\n",
    "        }\n",
    "\n",
    "        return img_t, mask_t, meta\n",
    "\n",
    "\n",
    "# Quick sanity check with a small DataLoader\n",
    "seg_dataset = DentalSegmentationDataset(df_seg=df_seg, transform=seg_train_transform)\n",
    "seg_loader = DataLoader(seg_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "batch_images, batch_masks, batch_meta = next(iter(seg_loader))\n",
    "print(\"Segmentation batch images shape:\", batch_images.shape)  # (B, 1, H, W)\n",
    "print(\"Segmentation batch masks shape :\", batch_masks.shape)   # (B, H, W)\n",
    "\n",
    "# batch_meta is a dict of lists (one list per key)\n",
    "# Let's show the meta information for the first sample in the batch:\n",
    "example_meta = {k: v[0] for k, v in batch_meta.items()}\n",
    "print(\"Example meta (first sample):\", example_meta)\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. PyTorch Dataset for pretraining (Stage 1)\n",
    "# \n",
    "# For Stage 1 pretraining, we want to:\n",
    "# \n",
    "# - Use **all** images in `df_pretrain` (all non-mask rows).\n",
    "# - For each image, generate **two** augmented views:\n",
    "#   * `view1` (weak augmentation)  \n",
    "#   * `view2` (strong augmentation)\n",
    "# \n",
    "# We define `DentalPretrainDataset` that returns:\n",
    "# \n",
    "# ```python\n",
    "# img1, img2, meta = dataset[i]\n",
    "# ```\n",
    "# \n",
    "# where `img1` and `img2` are both `FloatTensor` of shape `(1, H, W)`."
   ],
   "id": "3b99e4b7ea041d0d",
   "execution_count": 30,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # %%\n",
    "# # ============================================================\n",
    "# # Cell 7 — Pretraining dataset (weak + strong views)\n",
    "# # ============================================================\n",
    "# \n",
    "# class DentalPretrainDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     PyTorch Dataset for self-supervised / contrastive pretraining.\n",
    "# \n",
    "#     Each item returns:\n",
    "#         img_weak:   FloatTensor (1, H, W)\n",
    "#         img_strong: FloatTensor (1, H, W)\n",
    "#         meta:       dict with auxiliary information\n",
    "#     \"\"\"\n",
    "# \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         df_pretrain: pd.DataFrame,\n",
    "#         weak_transform: A.Compose,\n",
    "#         strong_transform: A.Compose,\n",
    "#     ):\n",
    "#         self.df = df_pretrain.reset_index(drop=True)\n",
    "#         self.weak_transform = weak_transform\n",
    "#         self.strong_transform = strong_transform\n",
    "# \n",
    "#     def __len__(self) -> int:\n",
    "#         return len(self.df)\n",
    "# \n",
    "#     def __getitem__(self, idx: int):\n",
    "#         row = self.df.iloc[idx]\n",
    "# \n",
    "#         img_path = get_image_path_from_row(row)\n",
    "#         img = load_image_as_array(img_path)  # uint8, (H, W)\n",
    "# \n",
    "#         # Apply weak and strong transforms separately\n",
    "#         out_weak = self.weak_transform(image=img)\n",
    "#         img_weak = out_weak[\"image\"]   # FloatTensor (1, H, W)\n",
    "# \n",
    "#         out_strong = self.strong_transform(image=img)\n",
    "#         img_strong = out_strong[\"image\"]  # FloatTensor (1, H, W)\n",
    "# \n",
    "#         meta = {\n",
    "#             \"img_path\": str(img_path),\n",
    "#             \"pair_id\": row[\"pair_id\"],\n",
    "#             \"age_group\": row[\"age_group\"],\n",
    "#             \"label_status\": row[\"label_status\"],\n",
    "#         }\n",
    "# \n",
    "#         return img_weak, img_strong, meta\n",
    "# \n",
    "# \n",
    "# # Quick sanity check for pretraining dataset\n",
    "# pretrain_dataset = DentalPretrainDataset(\n",
    "#     df_pretrain=df_pretrain,\n",
    "#     weak_transform=ssl_weak_transform,\n",
    "#     strong_transform=ssl_strong_transform,\n",
    "# )\n",
    "# pretrain_loader = DataLoader(pretrain_dataset, batch_size=2, shuffle=True)\n",
    "# \n",
    "# weak_batch, strong_batch, meta_batch = next(iter(pretrain_loader))\n",
    "# print(\"Pretrain weak batch shape  :\", weak_batch.shape)   # (B, 1, H, W)\n",
    "# print(\"Pretrain strong batch shape:\", strong_batch.shape) # (B, 1, H, W)\n",
    "# print(\"Example meta:\", meta_batch[0])\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 8. Optional — Save augmented samples to disk\n",
    "# \n",
    "# In most cases, it is better to apply augmentations **on the fly** during training.\n",
    "# However, sometimes you may want to:\n",
    "# \n",
    "# - Generate a small set of augmented samples for debugging.\n",
    "# - Precompute a larger augmented dataset for experiments.\n",
    "# \n",
    "# The following cell demonstrates how to:\n",
    "# \n",
    "# - Iterate over the pretraining dataset.\n",
    "# - For each original image, save `N` augmented weak/strong pairs to disk.\n",
    "# \n",
    "# ⚠ **Warning:** Saving a full augmented dataset can consume a lot of disk space\n",
    "# (e.g., 4000 images × multiple augmentations per image)."
   ],
   "id": "91874432b46010bb",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:46:17.589241Z",
     "start_time": "2025-11-22T01:46:17.578726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## New Cell — Helper for MIM: online augmentation + patch masking\n",
    "# \n",
    "# This cell defines:\n",
    "# \n",
    "# - `patchify` / `unpatchify`: convert between images and patch sequences.\n",
    "# - `DentalMIMDataset`: a PyTorch Dataset that, for each image:\n",
    "#     1) loads the raw image,\n",
    "#     2) applies a light augmentation (Albumentations),\n",
    "#     3) splits it into patches,\n",
    "#     4) randomly masks a subset of patches,\n",
    "#     5) returns:\n",
    "#         - `masked_patches`  (input to the MIM model)\n",
    "#         - `mask`            (which patches were masked)\n",
    "#         - `target_patches`  (ground truth patches to reconstruct)\n",
    "#         - `meta`            (some metadata for debugging)\n",
    "\n",
    "# %%\n",
    "import math\n",
    "\n",
    "def patchify(img_t: torch.Tensor, patch_size: int):\n",
    "    \"\"\"\n",
    "    Convert an image tensor into a sequence of flattened patches.\n",
    "\n",
    "    Args:\n",
    "        img_t: FloatTensor of shape (C, H, W). In our case C=1.\n",
    "        patch_size: size of square patches, e.g. 16.\n",
    "\n",
    "    Returns:\n",
    "        patches: FloatTensor of shape (num_patches, C * patch_size * patch_size)\n",
    "        grid_hw: tuple (n_h, n_w) where:\n",
    "                 n_h = number of patches along height\n",
    "                 n_w = number of patches along width\n",
    "\n",
    "    Note:\n",
    "        We assume H and W are divisible by patch_size.\n",
    "    \"\"\"\n",
    "    assert img_t.ndim == 3, f\"Expected (C, H, W), got {img_t.shape}\"\n",
    "    C, H, W = img_t.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \\\n",
    "        f\"H and W must be divisible by patch_size, got H={H}, W={W}, patch_size={patch_size}\"\n",
    "\n",
    "    n_h = H // patch_size\n",
    "    n_w = W // patch_size\n",
    "\n",
    "    # Add batch dimension: (1, C, H, W)\n",
    "    img_t = img_t.unsqueeze(0)\n",
    "\n",
    "    # Unfold into patches: shape (1, C, n_h, n_w, patch_size, patch_size)\n",
    "    patches = img_t.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    # Rearrange to (1, n_h, n_w, C, patch_size, patch_size)\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
    "\n",
    "    # Flatten each patch to a vector: (num_patches, C * patch_size * patch_size)\n",
    "    patches = patches.view(n_h * n_w, C * patch_size * patch_size)\n",
    "\n",
    "    return patches, (n_h, n_w)\n",
    "\n",
    "\n",
    "def unpatchify(patches: torch.Tensor, grid_hw, patch_size: int, C: int = 1):\n",
    "    \"\"\"\n",
    "    Reconstruct an image tensor from a sequence of flattened patches.\n",
    "\n",
    "    This is mostly for debugging / visualization.\n",
    "\n",
    "    Args:\n",
    "        patches: FloatTensor of shape (num_patches, C * patch_size * patch_size)\n",
    "        grid_hw: tuple (n_h, n_w)\n",
    "        patch_size: patch size used in patchify\n",
    "        C: number of channels (1 for grayscale)\n",
    "\n",
    "    Returns:\n",
    "        img_t: FloatTensor of shape (C, H, W)\n",
    "    \"\"\"\n",
    "    n_h, n_w = grid_hw\n",
    "    num_patches = n_h * n_w\n",
    "    assert patches.shape[0] == num_patches, \\\n",
    "        f\"Expected {num_patches} patches, got {patches.shape[0]}\"\n",
    "\n",
    "    # Reshape back to (1, n_h, n_w, C, patch_size, patch_size)\n",
    "    patches = patches.view(1, n_h, n_w, C, patch_size, patch_size)\n",
    "\n",
    "    # Rearrange to (1, C, n_h, patch_size, n_w, patch_size)\n",
    "    patches = patches.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "\n",
    "    # Merge patch dimensions back to H and W: (1, C, H, W)\n",
    "    img = patches.view(1, C, n_h * patch_size, n_w * patch_size)\n",
    "\n",
    "    # Remove batch dimension: (C, H, W)\n",
    "    return img[0]\n",
    "\n",
    "\n",
    "class DentalMIMDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for Masked Image Modeling (MIM).\n",
    "\n",
    "    For each item, it:\n",
    "        1) loads a raw image from df_pretrain,\n",
    "        2) applies a light augmentation transform (e.g. get_ssl_weak_transform()),\n",
    "        3) splits the augmented image into flattened patches,\n",
    "        4) randomly masks a subset of patches,\n",
    "        5) returns:\n",
    "            - masked_patches: FloatTensor (num_patches, patch_dim)\n",
    "                              patches where masked positions are set to 0.\n",
    "            - mask:           BoolTensor  (num_patches,)   True = masked\n",
    "            - target_patches: FloatTensor (num_patches, patch_dim)\n",
    "                              original patches before masking (for loss).\n",
    "            - meta:           dict with metadata.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_pretrain: pd.DataFrame,\n",
    "        transform: A.Compose,\n",
    "        patch_size: int = 16,\n",
    "        mask_ratio: float = 0.6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df_pretrain: DataFrame containing non-mask images (like df_pretrain in Cell 1).\n",
    "            transform: Albumentations transform for light augmentation + normalization.\n",
    "                       For example: get_ssl_weak_transform().\n",
    "            patch_size: size of square patches in pixels (e.g. 16).\n",
    "            mask_ratio: fraction of patches to mask (0.0 ~ 1.0), e.g. 0.6.\n",
    "        \"\"\"\n",
    "        self.df = df_pretrain.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.patch_size = patch_size\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # 1) Load grayscale image as uint8 array (H, W)\n",
    "        img_path = get_image_path_from_row(row)\n",
    "        img = load_image_as_array(img_path)\n",
    "\n",
    "        # 2) Apply augmentation + normalization\n",
    "        #    transform should output a dict with key \"image\" as a FloatTensor (C, H, W)\n",
    "        transformed = self.transform(image=img)\n",
    "        img_t = transformed[\"image\"]          # FloatTensor, shape (1, H, W)\n",
    "        C, H, W = img_t.shape\n",
    "\n",
    "        # 3) Split into flattened patches\n",
    "        patches, grid_hw = patchify(img_t, self.patch_size)   # (num_patches, patch_dim)\n",
    "        num_patches, patch_dim = patches.shape\n",
    "\n",
    "        # 4) Generate a random binary mask over patches\n",
    "        num_mask = int(self.mask_ratio * num_patches)\n",
    "        # mask: BoolTensor, True means \"this patch is masked\"\n",
    "        mask = torch.zeros(num_patches, dtype=torch.bool)\n",
    "        perm = torch.randperm(num_patches)\n",
    "        mask[perm[:num_mask]] = True\n",
    "\n",
    "        # 5) Create masked patches (input) and target patches (for loss)\n",
    "        target_patches = patches.clone()                  # ground truth\n",
    "        masked_patches = patches.clone()\n",
    "        masked_patches[mask] = 0.0                        # here we simply zero out masked patches\n",
    "\n",
    "        meta = {\n",
    "            \"img_path\": str(img_path),\n",
    "            \"pair_id\": row[\"pair_id\"],\n",
    "            \"age_group\": row[\"age_group\"],\n",
    "            \"label_status\": row[\"label_status\"],\n",
    "            \"grid_hw\": grid_hw,           # (n_h, n_w) for potential unpatchify/visualization\n",
    "            \"H\": H,\n",
    "            \"W\": W,\n",
    "        }\n",
    "\n",
    "        return masked_patches, mask, target_patches, meta"
   ],
   "id": "d9cb7d3e62b0a456",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T01:53:20.218736Z",
     "start_time": "2025-11-22T01:53:20.196490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%  (Sanity check for DentalMIMDataset)\n",
    "\n",
    "# Use the weak SSL transform as the light pre-augmentation for MIM\n",
    "mim_transform = get_ssl_weak_transform()\n",
    "\n",
    "mim_dataset = DentalMIMDataset(\n",
    "    df_pretrain=df_pretrain,\n",
    "    transform=mim_transform,\n",
    "    patch_size=16,\n",
    "    mask_ratio=0.6,\n",
    ")\n",
    "\n",
    "print(\"Number of MIM samples:\", len(mim_dataset))\n",
    "\n",
    "masked_patches, mask_vec, target_patches, meta = mim_dataset[0]\n",
    "print(\"masked_patches shape :\", masked_patches.shape)   # (num_patches, patch_dim)\n",
    "print(\"mask_vec shape       :\", mask_vec.shape)         # (num_patches,)\n",
    "print(\"target_patches shape :\", target_patches.shape)   # (num_patches,)\n",
    "print(\"Example meta         :\", meta)\n"
   ],
   "id": "7733cb4c45a4c835",
   "execution_count": 32,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlcv)",
   "language": "python",
   "name": "dlcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
