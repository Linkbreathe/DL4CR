{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:43.584138Z",
     "start_time": "2025-11-27T08:27:43.580609Z"
    }
   },
   "source": [
    "# # Pretrain MIM with 2D U-Net on STS-2D-Tooth\n",
    "#\n",
    "# This notebook performs self-supervised pretraining on all 2D X-ray images\n",
    "# using a Masked Image Modeling (MIM) / Masked Autoencoder-style objective\n",
    "# with a 2D U-Net backbone.\n",
    "#\n",
    "# Pipeline:\n",
    "# 1. Load the preprocessed index (`sts2d_index.csv`) and build `df_pretrain`\n",
    "#    (all non-mask images, ~4000 samples).\n",
    "# 2. Define a light augmentation pipeline for MIM pretraining.\n",
    "# 3. Define a `DentalMIMReconstructionDataset`:\n",
    "#      - loads an image\n",
    "#      - applies augmentation\n",
    "#      - generates random block masks on the image\n",
    "#      - returns (masked_image, target_image, mask, meta)\n",
    "# 4. Define a small 2D U-Net model.\n",
    "# 5. Train the U-Net to reconstruct the original image from the masked image,\n",
    "#    with the loss computed mainly on masked regions (MIM-style).\n",
    "# 6. Save the pretrained U-Net weights to disk for later segmentation fine-tuning."
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:52.695020Z",
     "start_time": "2025-11-27T08:27:43.593037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %%\n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots inline if you are in Jupyter\n",
    "# %matplotlib inline\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (aligned with data_enhance.ipynb)\n",
    "# -----------------------------\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "PROCESSED_2D_DIR = DATA_ROOT / \"processed_2d\"\n",
    "INDEX_CSV = DATA_ROOT / \"sts2d_index.csv\"\n",
    "\n",
    "print(\"DATA_ROOT      :\", DATA_ROOT)\n",
    "print(\"PROCESSED_2D   :\", PROCESSED_2D_DIR)\n",
    "print(\"INDEX_CSV path :\", INDEX_CSV)\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ],
   "id": "897764a3640c424f",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:52.724033Z",
     "start_time": "2025-11-27T08:27:52.696025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 2. Load index and build `df_pretrain`\n",
    "#\n",
    "# - We load `sts2d_index.csv` generated in `preprocessing.ipynb` / `data_enhance.ipynb`.\n",
    "# - Each row corresponds to a PNG file with:\n",
    "#     * rel_path\n",
    "#     * age_group\n",
    "#     * label_status\n",
    "#     * is_mask\n",
    "#     * pair_id\n",
    "# - For MIM pretraining, we only need non-mask images (`is_mask == False`).\n",
    "\n",
    "# %%\n",
    "assert INDEX_CSV.exists(), f\"Index CSV not found: {INDEX_CSV}\"\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV)\n",
    "print(\"Full index shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# Split into images vs masks\n",
    "df_img = df[df[\"is_mask\"] == False].copy()\n",
    "df_mask = df[df[\"is_mask\"] == True].copy()\n",
    "\n",
    "print(\"\\nNumber of image rows:\", len(df_img))\n",
    "print(\"Number of mask rows :\", len(df_mask))\n",
    "\n",
    "# For MIM pretraining, we use ALL non-mask images\n",
    "df_pretrain = df_img.reset_index(drop=True)\n",
    "print(\"\\nPretraining dataframe shape:\", df_pretrain.shape)\n",
    "\n",
    "print(\"\\nValue counts — age_group:\")\n",
    "print(df_pretrain[\"age_group\"].value_counts())\n",
    "\n",
    "print(\"\\nValue counts — label_status:\")\n",
    "print(df_pretrain[\"label_status\"].value_counts())\n"
   ],
   "id": "65755558aefb007e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:52.833929Z",
     "start_time": "2025-11-27T08:27:52.725084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 3. Helper functions\n",
    "#\n",
    "# We reuse the same logic as in `data_enhance.ipynb` to map index rows to\n",
    "# actual file paths under `processed_2d/`, and to load grayscale images.\n",
    "#\n",
    "# Folder structure (from preprocessing):\n",
    "#   processed_2d/\n",
    "#       adult/\n",
    "#         labeled/\n",
    "#           images/\n",
    "#           masks/\n",
    "#         unlabeled/\n",
    "#           images/\n",
    "#       children/\n",
    "#         ...\n",
    "# %%\n",
    "def get_image_path_from_row(row: pd.Series) -> Path:\n",
    "    \"\"\"\n",
    "    Map a row from df_pretrain to a local image path in processed_2d.\n",
    "\n",
    "    We ignore the original `rel_path` subfolder structure and use:\n",
    "        processed_2d / age_group / label_status / images / <filename>\n",
    "\n",
    "    For example:\n",
    "      rel_path = \"A-PXI/Labeled/Image/A_L_001.png\"\n",
    "      age_group = \"adult\"\n",
    "      label_status = \"labeled\"\n",
    "\n",
    "    -> processed_2d/adult/labeled/images/A_L_001.png\n",
    "    \"\"\"\n",
    "    rel = Path(row[\"rel_path\"])\n",
    "    age_group = row[\"age_group\"]\n",
    "    label_status = row[\"label_status\"]\n",
    "\n",
    "    # Handle potential \"unknown\" but in df_pretrain it should be adult/children + labeled/unlabeled\n",
    "    if age_group not in (\"adult\", \"children\"):\n",
    "        age_group = \"unknown\"\n",
    "    if label_status not in (\"labeled\", \"unlabeled\"):\n",
    "        label_status = \"unknown\"\n",
    "\n",
    "    img_path = PROCESSED_2D_DIR / age_group / label_status / \"images\" / rel.name\n",
    "    return img_path\n",
    "\n",
    "\n",
    "def load_image_as_array(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load a PNG as a grayscale numpy array of shape (H, W), dtype uint8.\n",
    "\n",
    "    We do NOT normalize or resize here. That will be handled by Albumentations.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Image file not found: {path}\")\n",
    "    img = Image.open(path).convert(\"L\")  # \"L\" = single-channel grayscale\n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "# Quick sanity check: try loading one random image\n",
    "sample_row = df_pretrain.sample(1, random_state=SEED).iloc[0]\n",
    "sample_path = get_image_path_from_row(sample_row)\n",
    "sample_img = load_image_as_array(sample_path)\n",
    "print(\"Sample image path:\", sample_path)\n",
    "print(\"Sample image shape:\", sample_img.shape)\n"
   ],
   "id": "f16b1299ec610e4d",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:52.840275Z",
     "start_time": "2025-11-27T08:27:52.834937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 4. Light augmentation for MIM pretraining\n",
    "#\n",
    "# We define a mild augmentation pipeline:\n",
    "# - Resize to a fixed size (320 x 640)\n",
    "# - Random horizontal flip\n",
    "# - Small rotation (to simulate slightly different acquisition angles)\n",
    "# - Mild brightness/contrast jitter\n",
    "# - Mild Gaussian noise and blur\n",
    "# - Normalize to mean=0.5, std=0.25 and convert to PyTorch tensor\n",
    "#\n",
    "# Important:\n",
    "# - We do *not* perform aggressive cropping or cutout here.\n",
    "# - Masking (MIM) will be handled separately by our Dataset.\n",
    "# %%\n",
    "TARGET_HEIGHT = 320\n",
    "TARGET_WIDTH = 640\n",
    "\n",
    "def get_mim_pretrain_transform() -> A.Compose:\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(\n",
    "                limit=5,\n",
    "                border_mode=cv2.BORDER_REFLECT_101,\n",
    "                p=0.5,\n",
    "            ),\n",
    "            # A.RandomBrightnessContrast(\n",
    "            #     brightness_limit=0.1,\n",
    "            #     contrast_limit=0.1,\n",
    "            #     p=0.5,\n",
    "            # ),\n",
    "            # A.GaussNoise(p=0.1),\n",
    "            # A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "            # A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "            ToTensorV2(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "mim_transform = get_mim_pretrain_transform()\n",
    "print(\"MIM pretrain transform created.\")"
   ],
   "id": "669e38277d824aa1",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:52.847282Z",
     "start_time": "2025-11-27T08:27:52.841283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 5. Random block mask generator\n",
    "#\n",
    "# For MIM-style pretraining with a 2D U-Net, we will:\n",
    "# - Work in pixel space (H, W), not in token space.\n",
    "# - Generate random block masks on the *image plane*:\n",
    "#     * Each mask is a rectangle with random size and position.\n",
    "#     * The union of all rectangles forms the final mask.\n",
    "# - The model will see the masked image and try to reconstruct the original.\n",
    "#\n",
    "# The function below creates a binary mask of shape (H, W) where:\n",
    "#   mask[y, x] = 1.0 means \"this pixel is masked (to be reconstructed)\".\n",
    "# %%\n",
    "\n",
    "def random_block_mask(\n",
    "    height: int,\n",
    "    width: int,\n",
    "    num_blocks: int = 8,\n",
    "    min_block_fraction: float = 0.2,\n",
    "    max_block_fraction: float = 0.25,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate a random block mask (H, W) in {0.0, 1.0},\n",
    "    where 1.0 = masked pixel.\n",
    "    \"\"\"\n",
    "    mask = np.zeros((height, width), dtype=np.float32)\n",
    "\n",
    "    for _ in range(num_blocks):\n",
    "        block_h = int(np.random.uniform(min_block_fraction, max_block_fraction) * height)\n",
    "        block_w = int(np.random.uniform(min_block_fraction, max_block_fraction) * width)\n",
    "\n",
    "        block_h = max(1, min(block_h, height))\n",
    "        block_w = max(1, min(block_w, width))\n",
    "\n",
    "        y1 = np.random.randint(0, max(1, height - block_h + 1))\n",
    "        x1 = np.random.randint(0, max(1, width - block_w + 1))\n",
    "\n",
    "        y2 = min(height, y1 + block_h)\n",
    "        x2 = min(width, x1 + block_w)\n",
    "\n",
    "        mask[y1:y2, x1:x2] = 1.0\n",
    "\n",
    "    return mask\n"
   ],
   "id": "5fd61169783165db",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:52.855432Z",
     "start_time": "2025-11-27T08:27:52.847282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## 6. `DentalMIMReconstructionDataset`\n",
    "#\n",
    "# For each image row in `df_pretrain`, this Dataset will:\n",
    "#\n",
    "# 1. Load the raw X-ray image (grayscale).\n",
    "# 2. Apply the MIM pretrain transform (resize + mild augment + normalize + ToTensor).\n",
    "#    - `image` is a FloatTensor of shape (1, H, W).\n",
    "# 3. Generate a random block mask in pixel coordinates:\n",
    "#    - `mask_np`: (H, W), float32 in {0, 1}\n",
    "# 4. Construct:\n",
    "#    - `target_img`  = the full augmented image (ground truth for reconstruction)\n",
    "#    - `masked_img`  = `target_img * (1 - mask)`  (we simply zero out masked pixels)\n",
    "#    - `mask_tensor` = mask as FloatTensor with shape (1, H, W)\n",
    "# 5. Return:\n",
    "#    - masked_img  (input to the MIM model)\n",
    "#    - target_img  (target for reconstruction)\n",
    "#    - mask_tensor (which pixels to focus the loss on)\n",
    "#    - meta        (some metadata: path, age_group, label_status, etc.)\n",
    "\n",
    "class DentalMIMReconstructionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df_pretrain: pd.DataFrame,\n",
    "        transform: A.Compose,\n",
    "        num_blocks: int = 8,\n",
    "        min_block_fraction: float = 0.2,\n",
    "        max_block_fraction: float = 0.25,\n",
    "    ):\n",
    "        self.df = df_pretrain.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.num_blocks = num_blocks\n",
    "        self.min_block_fraction = min_block_fraction\n",
    "        self.max_block_fraction = max_block_fraction\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = get_image_path_from_row(row)\n",
    "        img_np = load_image_as_array(img_path)  # (H, W) uint8\n",
    "\n",
    "        # 1) augment + normalize\n",
    "        transformed = self.transform(image=img_np)\n",
    "        img_tensor = transformed[\"image\"]  # (1, H, W), float32\n",
    "\n",
    "        # 2) block mask in pixel space\n",
    "        _, H, W = img_tensor.shape\n",
    "        mask_np = random_block_mask(\n",
    "            height=H,\n",
    "            width=W,\n",
    "            num_blocks=self.num_blocks,\n",
    "            min_block_fraction=self.min_block_fraction,\n",
    "            max_block_fraction=self.max_block_fraction,\n",
    "        )\n",
    "        mask_tensor = torch.from_numpy(mask_np).unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        # 3) construct masked input & target\n",
    "        target_img = img_tensor                      # full augmented image\n",
    "        masked_img = target_img * (1.0 - mask_tensor)  # zero out masked pixels\n",
    "\n",
    "        meta = {\n",
    "            \"img_path\": str(img_path),\n",
    "            \"pair_id\": row[\"pair_id\"],\n",
    "            \"age_group\": row[\"age_group\"],\n",
    "            \"label_status\": row[\"label_status\"],\n",
    "        }\n",
    "\n",
    "        return masked_img, target_img, mask_tensor, meta\n"
   ],
   "id": "b35a557f1f39a7b4",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:27:57.833610Z",
     "start_time": "2025-11-27T08:27:57.182363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 7. Sanity check: visualize MIM masking\n",
    "#\n",
    "# We sample a few images from the dataset and visualize:\n",
    "# - Original augmented image\n",
    "# - Masked image (same intensity scale as target)\n",
    "# - Mask map\n",
    "# %%\n",
    "mim_dataset = DentalMIMReconstructionDataset(\n",
    "    df_pretrain=df_pretrain,\n",
    "    transform=mim_transform,\n",
    "    num_blocks=8,\n",
    "    min_block_fraction=0.2,\n",
    "    max_block_fraction=0.25,\n",
    ")\n",
    "\n",
    "print(\"Number of MIM samples:\", len(mim_dataset))\n",
    "\n",
    "# Visualize a few samples\n",
    "NUM_VIS_SAMPLES = 3\n",
    "\n",
    "for i in range(NUM_VIS_SAMPLES):\n",
    "    masked_img, target_img, mask_tensor, meta = mim_dataset[i]\n",
    "\n",
    "    # Convert to numpy for visualization\n",
    "    # target_img: (1, H, W)\n",
    "    target_np = target_img.squeeze(0).cpu().numpy()\n",
    "    masked_np = masked_img.squeeze(0).cpu().numpy()\n",
    "    mask_np   = mask_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "    print(\"\\n=== Sample\", i, \"===\")\n",
    "    print(\"Image path :\", meta[\"img_path\"])\n",
    "    print(\"age_group  :\", meta[\"age_group\"])\n",
    "    print(\"label_status:\", meta[\"label_status\"])\n",
    "    print(\"range target :\", float(target_np.min()), \"->\", float(target_np.max()))\n",
    "    print(\"range masked :\", float(masked_np.min()), \"->\", float(masked_np.max()))\n",
    "    print(\"range mask   :\", float(mask_np.min()),   \"->\", float(mask_np.max()))\n",
    "\n",
    "    # Use the same intensity range for target and masked image\n",
    "    vmin, vmax = target_np.min(), target_np.max()\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "    axes[0].imshow(target_np, cmap=\"gray\")\n",
    "    axes[0].set_title(\"Augmented image\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(masked_np, cmap=\"gray\", vmin=vmin, vmax=vmax)\n",
    "    axes[1].set_title(\"Masked image (same scale)\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    axes[2].imshow(mask_np, cmap=\"hot\", vmin=0, vmax=1)\n",
    "    axes[2].set_title(\"Mask map (1 = masked)\")\n",
    "    axes[2].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "c0b948b62d1e3826",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:28:06.322615Z",
     "start_time": "2025-11-27T08:28:06.187676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 8. 2D U-Net model definition\n",
    "#\n",
    "# We implement a standard 2D U-Net:\n",
    "# - Encoder: four levels with downsampling by factor 2 at each level.\n",
    "# - Decoder: four upsampling levels with skip connections.\n",
    "# - Input:  1-channel grayscale image (1, 320, 640).\n",
    "# - Output: 1-channel reconstruction (1, 320, 640).\n",
    "#\n",
    "# This model will be pretrained with a masked reconstruction objective\n",
    "# and later reused as a backbone for tooth segmentation.\n",
    "\n",
    "# %%\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2d -> BN -> ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with MaxPool then DoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then DoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if use bilinear upsampling, reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1: from previous decoder level\n",
    "        # x2: from encoder skip connection\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        # pad x1 to have the same size as x2 if needed\n",
    "        x1 = nn.functional.pad(\n",
    "            x1,\n",
    "            [diffX // 2, diffX - diffX // 2,\n",
    "             diffY // 2, diffY - diffY // 2],\n",
    "        )\n",
    "\n",
    "        # concatenate along channels\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"Final 1x1 convolution to produce output channels\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    \"\"\"\n",
    "    2D U-Net backbone for MIM pretraining.\n",
    "\n",
    "    - in_channels:  1 (grayscale)\n",
    "    - out_channels: 1 (reconstructed grayscale)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=32, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, base_channels)\n",
    "        self.down1 = Down(base_channels, base_channels * 2)\n",
    "        self.down2 = Down(base_channels * 2, base_channels * 4)\n",
    "        self.down3 = Down(base_channels * 4, base_channels * 8)\n",
    "        self.down4 = Down(base_channels * 8, base_channels * 8)\n",
    "\n",
    "        self.up1 = Up(base_channels * 16, base_channels * 4, bilinear)\n",
    "        self.up2 = Up(base_channels * 8, base_channels * 2, bilinear)\n",
    "        self.up3 = Up(base_channels * 4, base_channels, bilinear)\n",
    "        self.up4 = Up(base_channels * 2, base_channels, bilinear)\n",
    "\n",
    "        self.outc = OutConv(base_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)      # (B, C, H, W)\n",
    "        x2 = self.down1(x1)   # (B, 2C, H/2, W/2)\n",
    "        x3 = self.down2(x2)   # (B, 4C, H/4, W/4)\n",
    "        x4 = self.down3(x3)   # (B, 8C, H/8, W/8)\n",
    "        x5 = self.down4(x4)   # (B, 8C, H/16, W/16)\n",
    "\n",
    "        # Decoder with skip connections\n",
    "        x = self.up1(x5, x4)  # (B, 4C, H/8,  W/8)\n",
    "        x = self.up2(x,  x3)  # (B, 2C, H/4,  W/4)\n",
    "        x = self.up3(x,  x2)  # (B, C,  H/2,  W/2)\n",
    "        x = self.up4(x,  x1)  # (B, C,  H,    W)\n",
    "\n",
    "        logits = self.outc(x) # (B, out_channels, H, W)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Instantiate model\n",
    "model = UNet2D(in_channels=1, out_channels=1, base_channels=32, bilinear=True)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n"
   ],
   "id": "124f103bf9bb4d0d",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:28:13.301943Z",
     "start_time": "2025-11-27T08:28:13.295901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 9. DataLoader and MIM loss\n",
    "#\n",
    "# - We build a DataLoader on top of `DentalMIMReconstructionDataset`.\n",
    "# - Loss: L1 reconstruction loss *on masked pixels only*:\n",
    "#     loss = mean( |pred - target| over masked regions )\n",
    "#\n",
    "# This is one simple version of MIM-style training. You can later experiment\n",
    "# with other losses (L2, SSIM, perceptual loss, etc.).\n",
    "\n",
    "\n",
    "# %%\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # adjust depending on your machine\n",
    "\n",
    "mim_dataset = DentalMIMReconstructionDataset(\n",
    "    df_pretrain=df_pretrain,\n",
    "    transform=mim_transform,\n",
    "    num_blocks=8,\n",
    "    min_block_fraction=0.2,\n",
    "    max_block_fraction=0.25,\n",
    ")\n",
    "\n",
    "mim_loader = DataLoader(\n",
    "    mim_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(\"MIM DataLoader is ready. Batches:\", len(mim_loader))\n",
    "\n",
    "\n",
    "def masked_l1_loss(pred: torch.Tensor, target: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute L1 loss on masked pixels only.\n",
    "\n",
    "    Args:\n",
    "        pred:   (B, 1, H, W)\n",
    "        target: (B, 1, H, W)\n",
    "        mask:   (B, 1, H, W) in {0,1}, where 1 = masked pixel\n",
    "\n",
    "    Returns:\n",
    "        scalar loss\n",
    "    \"\"\"\n",
    "    # element-wise L1\n",
    "    l1 = torch.abs(pred - target)  # (B, 1, H, W)\n",
    "\n",
    "    # focus only on masked positions\n",
    "    masked_l1 = l1 * mask\n",
    "\n",
    "    # avoid division by zero if mask is empty\n",
    "    denom = mask.sum()\n",
    "    if denom.item() < 1.0:\n",
    "        # fallback: full-image loss\n",
    "        return l1.mean()\n",
    "\n",
    "    loss = masked_l1.sum() / denom\n",
    "    return loss\n",
    "\n",
    "\n"
   ],
   "id": "77b41b68b3f09f23",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T08:28:17.361997Z",
     "start_time": "2025-11-27T08:28:15.706527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 10. Training hyperparameters and optimizer\n",
    "#\n",
    "# - Optimizer: Adam\n",
    "# - Learning rate: 5e-5 (you can tune this)\n",
    "# - Epochs: start small (e.g. 20) for a first run\n",
    "#\n",
    "# You can later increase the number of epochs if loss keeps decreasing.\n",
    "# %%\n",
    "LR = 5e-5\n",
    "NUM_EPOCHS = 50  # adjust based on your GPU / patience\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "print(\"Optimizer:\", optimizer)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# TensorBoard log directory\n",
    "LOG_DIR = DATA_ROOT / \"runs\" / \"mim_unet\"\n",
    "LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=str(LOG_DIR))\n",
    "print(\"TensorBoard log dir:\", LOG_DIR)\n",
    "\n",
    "# This will be used to index batch-wise logs\n",
    "global_step = 0"
   ],
   "id": "87c884e70861bea8",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T11:04:52.286284Z",
     "start_time": "2025-11-27T08:28:19.079191Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% 11. MIM pretraining loop with tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # By wrapping a dataloader in tqdm, you can see the progress and current loss of each batch.\n",
    "    pbar = tqdm(mim_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=True)\n",
    "\n",
    "    for batch in pbar:\n",
    "        masked_img, target_img, mask_tensor, meta = batch\n",
    "        # masked_img, target_img, mask_tensor: (B, 1, H, W)\n",
    "\n",
    "        masked_img = masked_img.to(device)\n",
    "        target_img = target_img.to(device)\n",
    "        mask_tensor = mask_tensor.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        pred = model(masked_img)  # (B, 1, H, W)\n",
    "\n",
    "        # Loss on masked pixels\n",
    "        loss = masked_l1_loss(pred, target_img, mask_tensor)\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        writer.add_scalar(\"train/batch_loss\", loss.item(), global_step)\n",
    "        global_step += 1\n",
    "        \n",
    "        # The progress bar displays the current batch loss.\n",
    "        pbar.set_postfix({\"batch_loss\": loss.item()})\n",
    "\n",
    "    avg_loss = running_loss / max(1, num_batches)\n",
    "    print(f\"Epoch [{epoch:02d}/{NUM_EPOCHS}] - Avg MIM loss: {avg_loss:.6f}\")\n",
    "    writer.add_scalar('Loss/Train', avg_loss, epoch)\n",
    "    \n",
    "# save the weights\n",
    "SAVE_DIR = DATA_ROOT / \"checkpoints\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_path = SAVE_DIR / \"unet2d_mim_pretrained.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "\n",
    "print(\"Pretrained U-Net weights saved to:\", save_path)"
   ],
   "id": "ca3062c76fef0495",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T11:20:01.933858Z",
     "start_time": "2025-11-27T11:20:01.863404Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = UNet2D(in_channels=1, out_channels=1, base_channels=32, bilinear=True)\n",
    "model = model.to(device)\n",
    "\n",
    "ckpt_path = DATA_ROOT / \"checkpoints\" / \"unet2d_mim_pretrained.pth\"\n",
    "state_dict = torch.load(ckpt_path, map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "print(\"Loaded pretrained weights from:\", ckpt_path)"
   ],
   "id": "a71bba24cba1e97b",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T11:20:04.284481Z",
     "start_time": "2025-11-27T11:20:03.335545Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Cell 12 — Improved reconstruction preview:\n",
    "# show target, masked input, \"input + repaired patches\", pure reconstruction, and mask.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denorm_np(x_np, mean=0.5, std=0.25):\n",
    "    \"\"\"\n",
    "    De-normalize an image that was normalized with\n",
    "    A.Normalize(mean=(0.5,), std=(0.25,)).\n",
    "    Returns values clipped to [0, 1] for visualization.\n",
    "    \"\"\"\n",
    "    x = x_np * std + mean\n",
    "    x = np.clip(x, 0.0, 1.0)\n",
    "    return x\n",
    "\n",
    "model.eval()\n",
    "\n",
    "NUM_PREVIEW = 3\n",
    "with torch.no_grad():\n",
    "    for i in range(NUM_PREVIEW):\n",
    "        # dataset returns normalized tensors: (1, H, W)\n",
    "        masked_img, target_img, mask_tensor, meta = mim_dataset[i]\n",
    "\n",
    "        masked_img_b = masked_img.unsqueeze(0).to(device)  # (1, 1, H, W)\n",
    "        pred_b = model(masked_img_b)                       # (1, 1, H, W)\n",
    "\n",
    "        # to numpy (normalized space)\n",
    "        pred_img  = pred_b.squeeze(0).squeeze(0).cpu().numpy()\n",
    "        target_np = target_img.squeeze(0).cpu().numpy()\n",
    "        masked_np = masked_img.squeeze(0).cpu().numpy()\n",
    "        mask_np   = mask_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "        print(f\"\\n=== Reconstruction preview sample {i} ===\")\n",
    "        print(\"Image path :\", meta[\"img_path\"])\n",
    "        print(\"age_group  :\", meta[\"age_group\"])\n",
    "        print(\"label_status:\", meta[\"label_status\"])\n",
    "        print(\"target_np min/max :\", float(target_np.min()), float(target_np.max()))\n",
    "        print(\"masked_np min/max :\", float(masked_np.min()), float(masked_np.max()))\n",
    "        print(\"mask_np   min/max :\", float(mask_np.min()), float(mask_np.max()))\n",
    "        print(\"pred_img  min/max :\", float(pred_img.min()), float(pred_img.max()))\n",
    "\n",
    "        # ---- 关键：在“归一化空间”里合成 input + repaired patches ----\n",
    "        # X = masked input (已经等于 target * (1 - mask))\n",
    "        # P = pred_img\n",
    "        # 我们想要的 inpaint 结果 = X + P * mask\n",
    "        combined_norm = masked_np + pred_img * mask_np\n",
    "        combined_vis  = denorm_np(combined_norm)\n",
    "\n",
    "        # de-normalize for nicer visualization\n",
    "        target_vis = denorm_np(target_np)\n",
    "        masked_vis = denorm_np(masked_np)\n",
    "        pred_vis   = denorm_np(pred_img)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 4, figsize=(22, 4))\n",
    "\n",
    "        axes[0].imshow(target_vis, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[0].set_title(\"Target (denorm)\")\n",
    "        axes[0].axis(\"off\")\n",
    "\n",
    "        axes[1].imshow(masked_vis, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[1].set_title(\"Masked input (denorm)\")\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "        axes[2].imshow(combined_vis, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        axes[2].set_title(\"Input + repaired patches\")\n",
    "        axes[2].axis(\"off\")\n",
    "\n",
    "        # axes[3].imshow(pred_vis, cmap=\"gray\", vmin=0, vmax=1)\n",
    "        # axes[3].set_title(\"Pure reconstruction (denorm)\")\n",
    "        # axes[3].axis(\"off\")\n",
    "\n",
    "        axes[3].imshow(mask_np, cmap=\"hot\", vmin=0, vmax=1)\n",
    "        axes[3].set_title(\"Mask (1 = masked)\")\n",
    "        axes[3].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ],
   "id": "914e122d08306f9d",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T19:36:10.923869Z",
     "start_time": "2025-11-22T19:36:10.919473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load existing weights\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"len(mim_dataset):\", len(mim_dataset))\n",
    "\n",
    "MIM_BATCH_SIZE = 4\n",
    "MIM_NUM_WORKERS = 0\n",
    "\n",
    "mim_loader = DataLoader(\n",
    "    mim_dataset,                      # DentalMIMReconstructionDataset\n",
    "    batch_size=MIM_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=MIM_NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(\"mim_loader ready, num_batches:\", len(mim_loader))\n"
   ],
   "id": "b61352f0483e235a",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-22T18:57:40.754576Z",
     "start_time": "2025-11-22T18:29:20.444586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# ====== Path & Device ======\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "CHECKPOINT_DIR = DATA_ROOT / \"checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ckpt_path = CHECKPOINT_DIR / \"unet2d_mim_pretrained.pth\"\n",
    "print(\"Checkpoint path:\", ckpt_path)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# ====== 1. Construct a U-Net with the same structure and load existing weights. ======\n",
    "model = UNet2D(in_channels=1, out_channels=1, base_channels=32, bilinear=True).to(device)\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=device)\n",
    "\n",
    "if isinstance(state, dict) and \"model_state_dict\" in state:\n",
    "    model.load_state_dict(state[\"model_state_dict\"])\n",
    "    start_epoch = state.get(\"epoch\", 0) + 1\n",
    "    print(f\"Loaded model_state_dict from checkpoint, resume from epoch ~{start_epoch}\")\n",
    "else:\n",
    "    model.load_state_dict(state)\n",
    "    start_epoch = 1\n",
    "    print(\"Loaded plain state_dict from checkpoint, start_epoch set to 1 (for logging only)\")\n",
    "\n",
    "# ====== 2. Optimizer ======\n",
    "LR = 1e-5   # If the loss doesn't decrease significantly, you can try changing it to 5e-5.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "if isinstance(state, dict) and \"optimizer_state_dict\" in state:\n",
    "    try:\n",
    "        optimizer.load_state_dict(state[\"optimizer_state_dict\"])\n",
    "        print(\"Loaded optimizer_state_dict from checkpoint.\")\n",
    "    except Exception as e:\n",
    "        print(\"Optimizer state in checkpoint not compatible, re-init optimizer. Error:\", e)\n",
    "\n",
    "# ====== 3. Continue training for several more epochs (with the TQDM progress bar). ======\n",
    "EXTRA_EPOCHS = 10  \n",
    "\n",
    "for epoch in range(start_epoch, start_epoch + EXTRA_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    \n",
    "    pbar = tqdm(mim_loader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    for masked_img, target_img, mask_tensor, meta in pbar:\n",
    "        masked_img = masked_img.to(device)   # (B, 1, H, W)\n",
    "        target_img = target_img.to(device)   # (B, 1, H, W)\n",
    "        mask_tensor = mask_tensor.to(device) # (B, 1, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(masked_img)\n",
    "\n",
    "        \n",
    "        loss = masked_l1_loss(pred, target_img, mask_tensor)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        avg_batch_loss = running_loss / num_batches\n",
    "        pbar.set_postfix({\"loss\": f\"{avg_batch_loss:.6f}\"})\n",
    "\n",
    "    avg_epoch_loss = running_loss / max(1, num_batches)\n",
    "    print(f\"[Resume] Epoch {epoch} - avg MIM train loss: {avg_epoch_loss:.6f}\")\n",
    "\n",
    "    # Save the latest weights once per epoch (overwriting the original .pth file).\n",
    "    torch.save(model.state_dict(), ckpt_path)\n",
    "\n",
    "print(\"Resume training done. Latest weights saved to:\", ckpt_path)"
   ],
   "id": "77a4934d9162083d",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": " !tensorboard --logdir \"E:\\Data\\ToothSeg\\sts_tooth_data\\runs\"",
   "id": "ea92d01bfdb67c62",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlcv)",
   "language": "python",
   "name": "dlcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
