{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:47.896108Z",
     "start_time": "2025-11-27T22:57:42.208211Z"
    }
   },
   "source": [
    "# full-image reconstruction with randomly occluded input\n",
    "\n",
    "# %% \n",
    "import os\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# %matplotlib inline  # uncomment in Jupyter if needed\n",
    "\n",
    "# -----------------------------\n",
    "# Reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (aligned with previous notebooks)\n",
    "# -----------------------------\n",
    "DATA_ROOT = Path(\"./sts_tooth_data\").resolve()\n",
    "PROCESSED_2D_DIR = DATA_ROOT / \"processed_2d\"\n",
    "INDEX_CSV = DATA_ROOT / \"sts2d_index.csv\"\n",
    "CHECKPOINT_DIR = DATA_ROOT / \"checkpoints\"\n",
    "\n",
    "print(\"DATA_ROOT      :\", DATA_ROOT)\n",
    "print(\"PROCESSED_2D   :\", PROCESSED_2D_DIR)\n",
    "print(\"INDEX_CSV path :\", INDEX_CSV)\n",
    "print(\"CHECKPOINT_DIR :\", CHECKPOINT_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# Device\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:47.926124Z",
     "start_time": "2025-11-27T22:57:47.897112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## 1. Build `df_seg` (image–mask pairs)\n",
    "#\n",
    "# The index `sts2d_index.csv` has columns:\n",
    "# - rel_path\n",
    "# - age_group\n",
    "# - label_status\n",
    "# - is_mask\n",
    "# - pair_id\n",
    "#\n",
    "# We will:\n",
    "# - separate image rows (is_mask == False) and mask rows (is_mask == True),\n",
    "# - inner-join them on `pair_id` to obtain only pairs that have both image and mask:\n",
    "#     -> `df_seg` with ~900 rows.\n",
    "# %%\n",
    "assert INDEX_CSV.exists(), f\"Index CSV not found: {INDEX_CSV}\"\n",
    "\n",
    "df = pd.read_csv(INDEX_CSV)\n",
    "print(\"Full index shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "df_img = df[df[\"is_mask\"] == False].copy()\n",
    "df_mask = df[df[\"is_mask\"] == True].copy()\n",
    "\n",
    "print(\"\\nNumber of image rows:\", len(df_img))\n",
    "print(\"Number of mask rows :\", len(df_mask))\n",
    "\n",
    "# Keep only the columns we need from the mask df for joining\n",
    "df_mask_simple = df_mask[[\"pair_id\", \"rel_path\"]].rename(columns={\"rel_path\": \"mask_rel\"})\n",
    "\n",
    "# Inner join on pair_id to obtain only (image, mask) pairs\n",
    "df_seg = pd.merge(df_img, df_mask_simple, on=\"pair_id\", how=\"inner\")\n",
    "\n",
    "print(\"\\nSegmentation dataframe shape:\", df_seg.shape)\n",
    "print(df_seg.head())\n",
    "\n",
    "print(\"\\nValue counts — age_group in df_seg:\")\n",
    "print(df_seg[\"age_group\"].value_counts())\n",
    "\n",
    "print(\"\\nValue counts — label_status in df_seg:\")\n",
    "print(df_seg[\"label_status\"].value_counts())"
   ],
   "id": "1ffffed0a61ee26d",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:47.932406Z",
     "start_time": "2025-11-27T22:57:47.926124Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 2. Train/val/test split\n",
    "#\n",
    "# We will:\n",
    "# - Shuffle `df_seg`,\n",
    "# - Split into:\n",
    "#     - ~70% train\n",
    "#     - ~15% val\n",
    "#     - ~15% test\n",
    "#\n",
    "# For simplicity, we perform a random split without explicit stratification.\n",
    "# (If needed, we could later add stratified splitting based on age_group and label_status.)\n",
    "# %%\n",
    "df_seg_shuffled = df_seg.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "n_total = len(df_seg_shuffled)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "n_test = n_total - n_train - n_val\n",
    "\n",
    "df_train = df_seg_shuffled.iloc[:n_train].reset_index(drop=True)\n",
    "df_val   = df_seg_shuffled.iloc[n_train:n_train + n_val].reset_index(drop=True)\n",
    "df_test  = df_seg_shuffled.iloc[n_train + n_val:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Total samples: {n_total}\")\n",
    "print(f\"Train: {len(df_train)}, Val: {len(df_val)}, Test: {len(df_test)}\")"
   ],
   "id": "844aec4056ca60d0",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:47.941105Z",
     "start_time": "2025-11-27T22:57:47.932406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## 3. Helper functions for image and mask paths\n",
    "#\n",
    "# We convert `rel_path` and `mask_rel` into actual file paths under\n",
    "# `processed_2d/` using the same convention as in `preprocessing.ipynb`:\n",
    "#\n",
    "# - Images:\n",
    "#     processed_2d / age_group / label_status / \"images\" / <filename>\n",
    "# - Masks:\n",
    "#     processed_2d / age_group / label_status / \"masks\"  / <filename>\n",
    "# %%\n",
    "def get_image_path_from_row(row: pd.Series) -> Path:\n",
    "    rel = Path(row[\"rel_path\"])\n",
    "    age_group = row[\"age_group\"]\n",
    "    label_status = row[\"label_status\"]\n",
    "\n",
    "    if age_group not in (\"adult\", \"children\"):\n",
    "        age_group = \"unknown\"\n",
    "    if label_status not in (\"labeled\", \"unlabeled\"):\n",
    "        label_status = \"unknown\"\n",
    "\n",
    "    img_path = PROCESSED_2D_DIR / age_group / label_status / \"images\" / rel.name\n",
    "    return img_path\n",
    "\n",
    "\n",
    "def get_mask_path_from_row(row: pd.Series) -> Path:\n",
    "    rel = Path(row[\"mask_rel\"])\n",
    "    age_group = row[\"age_group\"]\n",
    "    label_status = row[\"label_status\"]\n",
    "\n",
    "    if age_group not in (\"adult\", \"children\"):\n",
    "        age_group = \"unknown\"\n",
    "    if label_status not in (\"labeled\", \"unlabeled\"):\n",
    "        label_status = \"unknown\"\n",
    "\n",
    "    mask_path = PROCESSED_2D_DIR / age_group / label_status / \"masks\" / rel.name\n",
    "    return mask_path\n",
    "\n",
    "\n",
    "def load_image_as_array(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load a PNG as a grayscale numpy array of shape (H, W), dtype uint8.\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Image file not found: {path}\")\n",
    "    img = Image.open(path).convert(\"L\")  # grayscale\n",
    "    return np.array(img)\n",
    "\n",
    "\n",
    "def load_mask_as_array(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load a PNG as a mask numpy array of shape (H, W), dtype uint8.\n",
    "    We will later binarize this mask (foreground vs background).\n",
    "    \"\"\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Mask file not found: {path}\")\n",
    "    mask = Image.open(path).convert(\"L\")\n",
    "    return np.array(mask)\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "sample_row = df_train.iloc[0]\n",
    "print(\"\\nSanity check paths:\")\n",
    "print(\"Image path:\", get_image_path_from_row(sample_row))\n",
    "print(\"Mask path :\", get_mask_path_from_row(sample_row))\n"
   ],
   "id": "86016a24916aeace",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:47.951110Z",
     "start_time": "2025-11-27T22:57:47.942110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 4. Segmentation transforms (Albumentations)\n",
    "#\n",
    "# We define:\n",
    "# - `get_segmentation_train_transform()`:\n",
    "#     * Resize to (320, 640)\n",
    "#     * Horizontal flip\n",
    "#     * Small rotation\n",
    "#     * Mild brightness/contrast, noise, blur\n",
    "#     * Normalize (mean=0.5, std=0.25)\n",
    "#     * Convert to tensor\n",
    "#\n",
    "# - `get_segmentation_val_transform()`:\n",
    "#     * Resize + normalize + tensor\n",
    "#\n",
    "# Masks are not normalized by A.Normalize. ToTensorV2 converts them to tensors, \n",
    "# and we explicitly binarize with (mask > 0.5) afterward.\n",
    "\n",
    "# We will binarize them after the transform.\n",
    "# %%\n",
    "TARGET_HEIGHT = 320\n",
    "TARGET_WIDTH = 640\n",
    "\n",
    "def get_segmentation_train_transform() -> A.Compose:\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Rotate(\n",
    "                limit=5,\n",
    "                border_mode=cv2.BORDER_REFLECT_101,\n",
    "                p=0.5,\n",
    "            ),\n",
    "            # A.RandomBrightnessContrast(\n",
    "            #     brightness_limit=0.1,\n",
    "            #     contrast_limit=0.1,\n",
    "            #     p=0.5,\n",
    "            # ),\n",
    "            # A.GaussNoise(p=0.2),\n",
    "            # A.GaussianBlur(blur_limit=(3, 5), p=0.2),\n",
    "            # A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={\"mask\": \"mask\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def get_segmentation_val_transform() -> A.Compose:\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=TARGET_HEIGHT, width=TARGET_WIDTH),\n",
    "            # A.ToFloat(max_value=255.0),\n",
    "            A.Normalize(mean=(0.5,), std=(0.25,)),\n",
    "            ToTensorV2(),\n",
    "        ],\n",
    "        additional_targets={\"mask\": \"mask\"},\n",
    "    )\n",
    "\n",
    "\n",
    "train_transform = get_segmentation_train_transform()\n",
    "val_transform = get_segmentation_val_transform()\n",
    "print(\"Segmentation transforms created.\")"
   ],
   "id": "be2c3e9bab569e7a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:48.053088Z",
     "start_time": "2025-11-27T22:57:47.951614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 5. `DentalSegmentationDataset`\n",
    "#\n",
    "# This Dataset:\n",
    "# - For each row in df_train/df_val/df_test:\n",
    "#     * loads grayscale image and mask,\n",
    "#     * applies Albumentations transform (with consistent transform on image+mask),\n",
    "#     * binarizes mask to {0,1},\n",
    "#     * returns `(image_tensor, mask_tensor, meta_dict)`.\n",
    "# %%\n",
    "class DentalSegmentationDataset(Dataset):\n",
    "    def __init__(self, df_seg: pd.DataFrame, transform: A.Compose):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df_seg: DataFrame containing image–mask pairs.\n",
    "            transform: Albumentations transform (for both image and mask).\n",
    "        \"\"\"\n",
    "        self.df = df_seg.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_path = get_image_path_from_row(row)\n",
    "        mask_path = get_mask_path_from_row(row)\n",
    "\n",
    "        img_np = load_image_as_array(img_path)   # (H, W), uint8\n",
    "        mask_np = load_mask_as_array(mask_path)  # (H, W), uint8\n",
    "\n",
    "        # Albumentations expects dict with \"image\" and \"mask\"\n",
    "        augmented = self.transform(image=img_np, mask=mask_np)\n",
    "        img_t = augmented[\"image\"]  # FloatTensor, shape (1, H, W)\n",
    "        mask_t = augmented[\"mask\"]  # FloatTensor, shape (1, H, W) or (H, W)\n",
    "\n",
    "        # Ensure mask has shape (1, H, W)\n",
    "        if mask_t.ndim == 2:\n",
    "            mask_t = mask_t.unsqueeze(0)\n",
    "\n",
    "        # Binarize mask to {0,1}\n",
    "        mask_t = (mask_t > 0.5).float()\n",
    "\n",
    "        meta = {\n",
    "            \"img_path\": str(img_path),\n",
    "            \"mask_path\": str(mask_path),\n",
    "            \"pair_id\": row[\"pair_id\"],\n",
    "            \"age_group\": row[\"age_group\"],\n",
    "            \"label_status\": row[\"label_status\"],\n",
    "        }\n",
    "\n",
    "        return img_t, mask_t, meta\n",
    "\n",
    "\n",
    "# Quick sanity check: one sample\n",
    "train_ds = DentalSegmentationDataset(df_train, transform=train_transform)\n",
    "img_t, mask_t, meta = train_ds[0]\n",
    "print(\"\\nSample from train_ds:\")\n",
    "print(\"Image tensor shape:\", img_t.shape)\n",
    "print(\"Mask tensor shape :\", mask_t.shape)\n",
    "print(\"Meta:\", meta)\n"
   ],
   "id": "6798201ae3992162",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:48.058253Z",
     "start_time": "2025-11-27T22:57:48.054094Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 6. Dataloaders\n",
    "#\n",
    "# We create three dataloaders:\n",
    "# - train_loader\n",
    "# - val_loader\n",
    "# - test_loader\n",
    "#\n",
    "# Training loader is shuffled, validation/test loaders are not.\n",
    "# %%\n",
    "BATCH_SIZE = 4\n",
    "NUM_WORKERS = 0  # adjust depending on your machine\n",
    "\n",
    "train_ds = DentalSegmentationDataset(df_train, transform=train_transform)\n",
    "val_ds   = DentalSegmentationDataset(df_val,   transform=val_transform)\n",
    "test_ds  = DentalSegmentationDataset(df_test,  transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(device.type == \"cuda\"),\n",
    ")\n",
    "\n",
    "print(\"Dataloaders ready:\")\n",
    "print(\"  train batches:\", len(train_loader))\n",
    "print(\"  val batches  :\", len(val_loader))\n",
    "print(\"  test batches :\", len(test_loader))\n"
   ],
   "id": "c1ba29fdf70a164",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:48.172542Z",
     "start_time": "2025-11-27T22:57:48.059257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 7. 2D U-Net model (same as in pretrain_MIM.ipynb)\n",
    "#\n",
    "# We reuse the same 2D U-Net architecture:\n",
    "# - in_channels = 1  (grayscale)\n",
    "# - out_channels = 1 (binary segmentation logit)\n",
    "#\n",
    "# The only difference is the interpretation of the output: here it is a\n",
    "# per-pixel logit for \"tooth\" vs \"background\".\n",
    "# %%\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(Conv2d -> BN -> ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if mid_channels is None:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with MaxPool then DoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then DoubleConv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(\n",
    "                in_channels, in_channels // 2, kernel_size=2, stride=2\n",
    "            )\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # x1: from previous decoder level\n",
    "        # x2: from encoder skip connection\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = nn.functional.pad(\n",
    "            x1,\n",
    "            [diffX // 2, diffX - diffX // 2,\n",
    "             diffY // 2, diffY - diffY // 2],\n",
    "        )\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    \"\"\"Final 1x1 convolution to produce output channels\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet2D(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1, base_channels=32, bilinear=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.bilinear = bilinear\n",
    "        self.base_channels = base_channels\n",
    "\n",
    "        self.inc = DoubleConv(in_channels, base_channels)\n",
    "        self.down1 = Down(base_channels, base_channels * 2)\n",
    "        self.down2 = Down(base_channels * 2, base_channels * 4)\n",
    "        self.down3 = Down(base_channels * 4, base_channels * 8)\n",
    "        self.down4 = Down(base_channels * 8, base_channels * 8)\n",
    "\n",
    "        self.up1 = Up(base_channels * 16, base_channels * 4, bilinear)\n",
    "        self.up2 = Up(base_channels * 8, base_channels * 2, bilinear)\n",
    "        self.up3 = Up(base_channels * 4, base_channels, bilinear)\n",
    "        self.up4 = Up(base_channels * 2, base_channels, bilinear)\n",
    "\n",
    "        self.outc = OutConv(base_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x,  x3)\n",
    "        x = self.up3(x,  x2)\n",
    "        x = self.up4(x,  x1)\n",
    "\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = UNet2D(in_channels=1, out_channels=1, base_channels=32, bilinear=True)\n",
    "model = model.to(device)\n",
    "print(model)\n"
   ],
   "id": "1010cbce668d7a19",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:48.215850Z",
     "start_time": "2025-11-27T22:57:48.173547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 8. Load Pretrain weights (optional)\n",
    "#\n",
    "# We use the checkpoint from MIM pretraining:\n",
    "# - `unet2d_mim_pretrained.pth`\n",
    "#\n",
    "# Strategy:\n",
    "# - Build a UNet2D with the same architecture.\n",
    "# - Load the entire state dict.\n",
    "# - Optionally re-initialize the final `outc` layer (since the meaning\n",
    "#   of the output changes from \"reconstruction\" to \"segmentation logit\").\n",
    "#\n",
    "# By setting `USE_PRETRAINED = False`, you can train a baseline model\n",
    "# from scratch to compare with the MIM-pretrained initialization.\n",
    "# %%\n",
    "USE_PRETRAINED = True  # set to False for a from-scratch baseline\n",
    "\n",
    "# mim pretrain checkpoint path\n",
    "# ckpt_path = CHECKPOINT_DIR / \"unet2d_mim_pretrained.pth\"\n",
    "\n",
    "# full image pretrain checkpoint path\n",
    "ckpt_path = CHECKPOINT_DIR / \"unet2d_fullimage_pretrained.pth\"\n",
    "\n",
    "if USE_PRETRAINED and ckpt_path.exists():\n",
    "    state_dict = torch.load(ckpt_path, map_location=device)\n",
    "    missing, unexpected = model.load_state_dict(state_dict, strict=False)\n",
    "    print(\"Loaded MIM-pretrained weights from:\", ckpt_path)\n",
    "    print(\"Missing keys   :\", missing)\n",
    "    print(\"Unexpected keys:\", unexpected)\n",
    "\n",
    "    # Optionally re-init final output layer for segmentation\n",
    "    model.outc = OutConv(model.base_channels, 1).to(device)\n",
    "    print(\"Reinitialized final output layer (outc) for segmentation.\")\n",
    "else:\n",
    "    if USE_PRETRAINED:\n",
    "        print(\"WARNING: USE_PRETRAINED=True but checkpoint not found, training from scratch.\")\n",
    "    else:\n",
    "        print(\"Training from scratch (no pretrained weights loaded).\")\n"
   ],
   "id": "eaa50465f7cc7595",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:48.222208Z",
     "start_time": "2025-11-27T22:57:48.215850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Loss functions and metrics\n",
    "#\n",
    "# - Loss:\n",
    "#     * `BCEWithLogitsLoss` + Dice loss\n",
    "# - Metrics:\n",
    "#     * Dice coefficient (foreground)\n",
    "#     * IoU (Jaccard index)\n",
    "# %%\n",
    "bce_loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def dice_loss_from_logits(logits: torch.Tensor, targets: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Soft Dice loss computed from logits and target masks in {0,1}.\n",
    "    Args:\n",
    "        logits:  (B, 1, H, W)\n",
    "        targets: (B, 1, H, W) in {0,1}\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    # Flatten per batch\n",
    "    probs_flat = probs.view(probs.size(0), -1)\n",
    "    targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "    intersection = (probs_flat * targets_flat).sum(dim=1)\n",
    "    union = probs_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
    "\n",
    "    dice = (2.0 * intersection + eps) / (union + eps)\n",
    "    return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "def combined_segmentation_loss(logits: torch.Tensor, targets: torch.Tensor, lambda_dice: float = 0.2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Combined loss: BCEWithLogits + lambda_dice * DiceLoss\n",
    "    \"\"\"\n",
    "    loss_bce = bce_loss_fn(logits, targets)\n",
    "    loss_dice = dice_loss_from_logits(logits, targets)\n",
    "    return loss_bce + lambda_dice * loss_dice\n",
    "\n",
    "\n",
    "def compute_dice_score(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, eps: float = 1e-6) -> float:\n",
    "    \"\"\"\n",
    "    Compute foreground Dice coefficient from logits and targets in {0,1}.\n",
    "    Returns a scalar (mean over batch).\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= threshold).float()\n",
    "\n",
    "    preds_flat = preds.view(preds.size(0), -1)\n",
    "    targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "    intersection = (preds_flat * targets_flat).sum(dim=1)\n",
    "    union = preds_flat.sum(dim=1) + targets_flat.sum(dim=1)\n",
    "\n",
    "    dice = (2.0 * intersection + eps) / (union + eps)\n",
    "    return dice.mean().item()\n",
    "\n",
    "\n",
    "def compute_iou_score(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, eps: float = 1e-6) -> float:\n",
    "    \"\"\"\n",
    "    Compute IoU (Jaccard index) from logits and targets in {0,1}.\n",
    "    Returns a scalar (mean over batch).\n",
    "    \"\"\"\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= threshold).float()\n",
    "\n",
    "    preds_flat = preds.view(preds.size(0), -1)\n",
    "    targets_flat = targets.view(targets.size(0), -1)\n",
    "\n",
    "    intersection = (preds_flat * targets_flat).sum(dim=1)\n",
    "    union = preds_flat.sum(dim=1) + targets_flat.sum(dim=1) - intersection\n",
    "\n",
    "    iou = (intersection + eps) / (union + eps)\n",
    "    return iou.mean().item()\n"
   ],
   "id": "60a553fcaa35a254",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:49.546655Z",
     "start_time": "2025-11-27T22:57:48.222208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 10. Optimizer and training hyperparameters\n",
    "#\n",
    "# - Optimizer: Adam\n",
    "# - LR: 1e-4 (smaller than MIM pretraining LR)\n",
    "# - Epochs: start with 50 as a reasonable number\n",
    "# - Early stopping: based on validation Dice, with a patience window.\n",
    "# %%\n",
    "LR = 5e-5\n",
    "NUM_EPOCHS = 100\n",
    "PATIENCE = 10  # epochs without val Dice improvement before early stopping\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "print(\"Optimizer:\", optimizer)\n"
   ],
   "id": "2454820df159f822",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:49.758193Z",
     "start_time": "2025-11-27T22:57:49.546655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Log path：sts_tooth_data/runs/seg_unet\n",
    "log_dir = DATA_ROOT / \"runs\" / \"seg_unet\"\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "writer = SummaryWriter(log_dir=str(log_dir))\n",
    "print(\"TensorBoard log dir:\", log_dir)"
   ],
   "id": "ac2fd796e899fe82",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T18:10:13.157001Z",
     "start_time": "2025-11-27T17:21:20.967262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 11. Fine-tuning loop (with tqdm + TensorBoard)\n",
    "#\n",
    "# For each epoch:\n",
    "# - Train:\n",
    "#     * forward on train batches\n",
    "#     * compute combined segmentation loss\n",
    "#     * backprop + update\n",
    "# - Validate:\n",
    "#     * compute loss, Dice, IoU on val set (no gradient)\n",
    "# - Track:\n",
    "#     * best validation Dice\n",
    "#     * save best model checkpoint\n",
    "# - Early stopping based on val Dice.\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "# best_ckpt_path = CHECKPOINT_DIR / (\"unet2d_seg_mim_best.pth\" if USE_PRETRAINED else \"unet2d_seg_scratch_best.pth\")\n",
    "best_ckpt_path = CHECKPOINT_DIR / (\"unet2d_seg_full_image_best.pth\" if USE_PRETRAINED else \"unet2d_seg_scratch_best.pth\")\n",
    "best_val_dice = 0.0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "global_step = 0 \n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    # -------------------------\n",
    "    # Training phase\n",
    "    \n",
    "    # -------------------------\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    train_batches = 0\n",
    "\n",
    "    # The tqdm progress bar is wrapped in a train_loader layer.\n",
    "    train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch:02d} [train]\", leave=False)\n",
    "    for batch in train_pbar:\n",
    "        imgs, masks, meta = batch\n",
    "        imgs = imgs.to(device)   # (B, 1, H, W)\n",
    "        masks = masks.to(device) # (B, 1, H, W)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = combined_segmentation_loss(logits, masks, lambda_dice=0.2)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        train_batches += 1\n",
    "        global_step += 1\n",
    "\n",
    "        avg_batch_loss = train_loss_sum / train_batches\n",
    "\n",
    "        # Update the loss displayed on the progress bar.\n",
    "        train_pbar.set_postfix({\"loss\": f\"{avg_batch_loss:.4f}\"})\n",
    "\n",
    "        # TensorBoard: Records training loss step by step.\n",
    "        writer.add_scalar(\"train/loss_step\", loss.item(), global_step)\n",
    "\n",
    "    avg_train_loss = train_loss_sum / max(1, train_batches)\n",
    "    writer.add_scalar(\"train/loss_epoch\", avg_train_loss, epoch)\n",
    "\n",
    "    # -------------------------\n",
    "    # Validation phase\n",
    "    # -------------------------\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    val_batches = 0\n",
    "    val_dice_sum = 0.0\n",
    "    val_iou_sum = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch:02d} [val]\", leave=False)\n",
    "        for batch in val_pbar:\n",
    "            imgs, masks, meta = batch\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            logits = model(imgs)\n",
    "            loss = combined_segmentation_loss(logits, masks, lambda_dice=0.2)\n",
    "\n",
    "            val_loss_sum += loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "            # metrics\n",
    "            batch_dice = compute_dice_score(logits, masks)\n",
    "            batch_iou  = compute_iou_score(logits, masks)\n",
    "\n",
    "            val_dice_sum += batch_dice\n",
    "            val_iou_sum  += batch_iou\n",
    "\n",
    "            avg_val_loss_tmp = val_loss_sum / max(1, val_batches)\n",
    "            avg_val_dice_tmp = val_dice_sum / max(1, val_batches)\n",
    "\n",
    "            # The verification progress bar displays the current average value loss / die.\n",
    "            val_pbar.set_postfix({\n",
    "                \"val_loss\": f\"{avg_val_loss_tmp:.4f}\",\n",
    "                \"val_dice\": f\"{avg_val_dice_tmp:.4f}\",\n",
    "            })\n",
    "\n",
    "    avg_val_loss = val_loss_sum / max(1, val_batches)\n",
    "    avg_val_dice = val_dice_sum / max(1, val_batches)\n",
    "    avg_val_iou  = val_iou_sum  / max(1, val_batches)\n",
    "\n",
    "    # TensorBoard: Records validation metrics by epoch.\n",
    "    writer.add_scalar(\"val/loss\", avg_val_loss, epoch)\n",
    "    writer.add_scalar(\"val/dice\", avg_val_dice, epoch)\n",
    "    writer.add_scalar(\"val/iou\",  avg_val_iou,  epoch)\n",
    "    writer.flush()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch:02d}/{NUM_EPOCHS}] \"\n",
    "        f\"- Train loss: {avg_train_loss:.4f} \"\n",
    "        f\"- Val loss: {avg_val_loss:.4f} \"\n",
    "        f\"- Val Dice: {avg_val_dice:.4f} \"\n",
    "        f\"- Val IoU: {avg_val_iou:.4f}\"\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Check for improvement (by val Dice)\n",
    "    # -------------------------\n",
    "    if avg_val_dice > best_val_dice:\n",
    "        best_val_dice = avg_val_dice\n",
    "        epochs_without_improvement = 0\n",
    "        torch.save(model.state_dict(), best_ckpt_path)\n",
    "        print(f\"  -> New best val Dice: {best_val_dice:.4f}. Saved checkpoint to {best_ckpt_path}\")\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "        print(f\"  -> No improvement in Dice for {epochs_without_improvement} epoch(s).\")\n",
    "\n",
    "    # Early stopping\n",
    "    if epochs_without_improvement >= PATIENCE:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "writer.close()\n"
   ],
   "id": "38b191b77453f76c",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:57:59.723620Z",
     "start_time": "2025-11-27T22:57:57.071531Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# %% [markdown]\n",
    "# ## 12. Evaluation on test set (using best checkpoint)\n",
    "#\n",
    "# We load the best checkpoint (according to validation Dice)\n",
    "# and evaluate on the held-out test set:\n",
    "# - Test loss\n",
    "# - Test Dice\n",
    "# - Test IoU\n",
    "# %%\n",
    "\n",
    "# Load best model for mim image training\n",
    "# best_ckpt_path = CHECKPOINT_DIR / (\"unet2d_seg_mim_best.pth\" if USE_PRETRAINED else \"unet2d_seg_scratch_best.pth\")\n",
    "\n",
    "# Load best model for full image training\n",
    "best_ckpt_path = CHECKPOINT_DIR / (\"unet2d_seg_full_image_best.pth\" if USE_PRETRAINED else \"unet2d_seg_scratch_best.pth\")\n",
    "\n",
    "if best_ckpt_path.exists():\n",
    "    state_dict = torch.load(best_ckpt_path, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Loaded best segmentation checkpoint from:\", best_ckpt_path)\n",
    "else:\n",
    "    print(\"WARNING: Best checkpoint not found, using current model weights.\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_loss_sum = 0.0\n",
    "test_batches = 0\n",
    "test_dice_sum = 0.0\n",
    "test_iou_sum = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        imgs, masks, meta = batch\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        loss = combined_segmentation_loss(logits, masks, lambda_dice=0.2)\n",
    "        test_loss_sum += loss.item()\n",
    "        test_batches += 1\n",
    "\n",
    "        batch_dice = compute_dice_score(logits, masks)\n",
    "        batch_iou  = compute_iou_score(logits, masks)\n",
    "\n",
    "        test_dice_sum += batch_dice\n",
    "        test_iou_sum  += batch_iou\n",
    "\n",
    "avg_test_loss = test_loss_sum / max(1, test_batches)\n",
    "avg_test_dice = test_dice_sum / max(1, test_batches)\n",
    "avg_test_iou  = test_iou_sum  / max(1, test_batches)\n",
    "\n",
    "print(\"\\n=== Test set evaluation ===\")\n",
    "print(f\"Test loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Test Dice: {avg_test_dice:.4f}\")\n",
    "print(f\"Test IoU : {avg_test_iou:.4f}\")\n"
   ],
   "id": "8a839c2bdddf6116",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T22:58:04.074976Z",
     "start_time": "2025-11-27T22:58:02.529858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ## 13. Qualitative visualization (optional)\n",
    "#\n",
    "# We visualize a few samples from the test set:\n",
    "# - Input X-ray\n",
    "# - Ground truth mask\n",
    "# - Predicted mask (thresholded at 0.5)\n",
    "# - Overlay\n",
    "# %%\n",
    "NUM_VIS_SAMPLES = 6\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    count = 0\n",
    "    for imgs, masks, meta in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        masks = masks.to(device)\n",
    "\n",
    "        logits = model(imgs)\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).float()\n",
    "\n",
    "        for b in range(imgs.size(0)):\n",
    "            if count >= NUM_VIS_SAMPLES:\n",
    "                break\n",
    "\n",
    "            img_np   = imgs[b].cpu().squeeze(0).numpy()\n",
    "            gt_np    = masks[b].cpu().squeeze(0).numpy()\n",
    "            pred_np  = preds[b].cpu().squeeze(0).numpy()\n",
    "\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "            axes[0].imshow(img_np, cmap=\"gray\")\n",
    "            axes[0].set_title(\"Input X-ray\")\n",
    "            axes[0].axis(\"off\")\n",
    "\n",
    "            axes[1].imshow(gt_np, cmap=\"gray\")\n",
    "            axes[1].set_title(\"Ground truth mask\")\n",
    "            axes[1].axis(\"off\")\n",
    "\n",
    "            axes[2].imshow(pred_np, cmap=\"gray\")\n",
    "            axes[2].set_title(\"Predicted mask\")\n",
    "            axes[2].axis(\"off\")\n",
    "\n",
    "            # Overlay prediction on input\n",
    "            axes[3].imshow(img_np, cmap=\"gray\")\n",
    "            axes[3].imshow(pred_np, alpha=0.4, cmap=\"jet\")\n",
    "            axes[3].set_title(\"Overlay (prediction)\")\n",
    "            axes[3].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            print(\"Image path:\", meta[\"img_path\"][b])\n",
    "            count += 1\n",
    "\n",
    "        if count >= NUM_VIS_SAMPLES:\n",
    "            break"
   ],
   "id": "671d800b1a666518",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-27T06:21:02.551663Z",
     "start_time": "2025-11-27T06:21:02.512357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images, masks, metas = next(iter(train_loader))\n",
    "\n",
    "print(\"images shape:\", images.shape)\n",
    "print(\"images dtype:\", images.dtype)\n",
    "print(\"images range:\", images.min().item(), \"->\", images.max().item())\n",
    "\n",
    "print(\"masks shape:\", masks.shape)\n",
    "print(\"masks dtype:\", masks.dtype)\n",
    "print(\"masks unique:\", masks.unique())\n",
    "\n",
    "print(\"metas type:\", type(metas))\n",
    "print(\"metas keys:\", metas.keys() if isinstance(metas, dict) else None)\n"
   ],
   "id": "faeb57608f6cb40e",
   "execution_count": 103,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "!tensorboard --logdir sts_tooth_data/runs",
   "id": "f1330749b981ad81",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dlcv)",
   "language": "python",
   "name": "dlcv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
